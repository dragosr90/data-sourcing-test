(bsrc-etl-v) PS C:\Users\B25712\bsrc-etl-venv\bsrc-etl> pytest test/test_extract/test_get_master_data.py
================================================================================================== test session starts ===================================================================================================
platform win32 -- Python 3.10.11, pytest-8.3.3, pluggy-1.5.0
rootdir: C:\Users\B25712\bsrc-etl-venv\bsrc-etl
configfile: pyproject.toml
plugins: cov-6.0.0, mock-3.14.0
collected 8 items

test\test_extract\test_get_master_data.py FF.F....                                                                                                                                                                  [100%]

======================================================================================================== FAILURES ======================================================================================================== 
_________________________________________________________________________________ test_full_master_data_sql[right_sources0-output_data0] _________________________________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x0000026137717640>, right_sources = ['TBL_B', 'TBL_C', 'TBL_C2'], output_data = [(1, 2, 3, 4, 1, 'A', ...), (1, 2, 3, 4, 1, 'A', ...)]

    @pytest.mark.parametrize(
        ("right_sources", "output_data"),
        [
            # All tables are joined from `JOINS` list, so 13 columns in total
            (
                ["TBL_B", "TBL_C", "TBL_C2"],
                [
                    (1, 2, 3, 4, 1, "A", "B", 1, "E", "F", 1, "C", "D"),
                    (1, 2, 3, 4, 1, "A", "B", 1, "C", "D", 1, "C", "D"),
                ],
            ),
            # Subset of tables is joined from `JOINS` list, so 10 columns in total
            (
                ["TBL_C", "TBL_C2"],
                [
                    (1, 2, 3, 4, 1, "E", "F", 1, "C", "D"),
                    (1, 2, 3, 4, 1, "C", "D", 1, "C", "D"),
                ],
            ),
            # One to one mapping, no transformations
            (
                ["TBL_A"],
                [
                    (1, 2, 3, 4),
                ],
            ),
            # Checking CASE statements
            (
                ["TBL_D_r", "TBL_D_l", "TBL_D"],
                [
                    (1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4),
                ],
            ),
        ],
    )
    def test_full_master_data_sql(spark_session, right_sources, output_data):
        """Test `get_full_master_data_sql`.

        In this test, all sources are loaded in temporary view.
        The total number of columns of the output dataset should match

        The number of columns specified in `SOURCES` & joined according
        to the configuration in `JOINS`.
        """
        for s in SOURCES:
            spark_session.createDataFrame(
                s["data"], schema=s["columns"]
            ).createOrReplaceTempView(s["source"])

        business_logic_dict = {
            "sources": SOURCES,
            "transformations": [
                {"join": j} for j in JOINS if j["right_source"] in right_sources
            ],
        }

        if len(business_logic_dict["transformations"]) == 0:
            business_logic_dict.pop("transformations", None)
        else:
            business_logic_dict["transformations"][0]["join"] = {
                "left_source": "TBL_A",
                **business_logic_dict["transformations"][0]["join"],
            }

        result = GetIntegratedData(
            spark_session,
            business_logic=business_logic_dict,
>       ).get_integrated_data()

test\test_extract\test_get_master_data.py:158:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  
src\extract\master_data_sql.py:34: in get_integrated_data
    return self.transform_data(data_dict=data_dict)
src\extract\master_data_sql.py:73: in transform_data
    transformed_data = self._apply_transformation(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  

self = <src.extract.master_data_sql.GetIntegratedData object at 0x00000261377178B0>, tf_step = ('join', {'condition': ['TBL_A.c1 = TBL_B.c1'], 'how': 'left', 'left_source': 'TBL_A', 'right_source': 'TBL_B'})
tf_params = {'condition': ['TBL_A.c1 = TBL_B.c1'], 'how': 'left', 'left_source': 'TBL_A', 'right_source': 'TBL_B'}, transformed_data = DataFrame[c1: bigint, c2: bigint, c3: bigint, c4: bigint]
data_dict = {'TBL_A': DataFrame[c1: bigint, c2: bigint, c3: bigint, c4: bigint], 'TBL_B': DataFrame[c1: bigint, c5: string, c7: st...'TBL_C': DataFrame[c1c: bigint, c6: string, c8: string], 'TBL_C2': DataFrame[c2c: bigint, c6: string, c8: string], ...}

    def _apply_transformation(
        self,
        tf_step: str,
        tf_params: dict,
        transformed_data: DataFrame,
        data_dict: dict[str, DataFrame],
    ) -> DataFrame:
        """Apply a specific transformation step to the data.

        Args:
            tf_step (str): Type of transformation (join, add_variables, etc.)
            tf_params (dict): Parameters for the transformation
            transformed_data (DataFrame): Current state of the data
            data_dict (dict[str, DataFrame]): Dictionary of all available data sources

        Returns:
            DataFrame: Data after applying the transformation
        """
        # Get required arguments for the transformation
>       kwgs = get_required_arguments({tf_step: tf_params}, self, tf_step)
E       TypeError: unhashable type: 'dict'

src\extract\master_data_sql.py:103: TypeError
------------------------------------------------------------------------------------------------- Captured stderr setup -------------------------------------------------------------------------------------------------- 
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
_________________________________________________________________________________ test_full_master_data_sql[right_sources1-output_data1] _________________________________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x0000026137717640>, right_sources = ['TBL_C', 'TBL_C2'], output_data = [(1, 2, 3, 4, 1, 'E', ...), (1, 2, 3, 4, 1, 'C', ...)]

    @pytest.mark.parametrize(
        ("right_sources", "output_data"),
        [
            # All tables are joined from `JOINS` list, so 13 columns in total
            (
                ["TBL_B", "TBL_C", "TBL_C2"],
                [
                    (1, 2, 3, 4, 1, "A", "B", 1, "E", "F", 1, "C", "D"),
                    (1, 2, 3, 4, 1, "A", "B", 1, "C", "D", 1, "C", "D"),
                ],
            ),
            # Subset of tables is joined from `JOINS` list, so 10 columns in total
            (
                ["TBL_C", "TBL_C2"],
                [
                    (1, 2, 3, 4, 1, "E", "F", 1, "C", "D"),
                    (1, 2, 3, 4, 1, "C", "D", 1, "C", "D"),
                ],
            ),
            # One to one mapping, no transformations
            (
                ["TBL_A"],
                [
                    (1, 2, 3, 4),
                ],
            ),
            # Checking CASE statements
            (
                ["TBL_D_r", "TBL_D_l", "TBL_D"],
                [
                    (1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4),
                ],
            ),
        ],
    )
    def test_full_master_data_sql(spark_session, right_sources, output_data):
        """Test `get_full_master_data_sql`.

        In this test, all sources are loaded in temporary view.
        The total number of columns of the output dataset should match

        The number of columns specified in `SOURCES` & joined according
        to the configuration in `JOINS`.
        """
        for s in SOURCES:
            spark_session.createDataFrame(
                s["data"], schema=s["columns"]
            ).createOrReplaceTempView(s["source"])

        business_logic_dict = {
            "sources": SOURCES,
            "transformations": [
                {"join": j} for j in JOINS if j["right_source"] in right_sources
            ],
        }

        if len(business_logic_dict["transformations"]) == 0:
            business_logic_dict.pop("transformations", None)
        else:
            business_logic_dict["transformations"][0]["join"] = {
                "left_source": "TBL_A",
                **business_logic_dict["transformations"][0]["join"],
            }

        result = GetIntegratedData(
            spark_session,
            business_logic=business_logic_dict,
>       ).get_integrated_data()

test\test_extract\test_get_master_data.py:158:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  
src\extract\master_data_sql.py:34: in get_integrated_data
    return self.transform_data(data_dict=data_dict)
src\extract\master_data_sql.py:73: in transform_data
    transformed_data = self._apply_transformation(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  

self = <src.extract.master_data_sql.GetIntegratedData object at 0x0000026137715D20>, tf_step = ('join', {'condition': ['TBL_A.c1 = TBL_C.c1c'], 'how': 'left', 'left_source': 'TBL_A', 'right_source': 'TBL_C'})
tf_params = {'condition': ['TBL_A.c1 = TBL_C.c1c'], 'how': 'left', 'left_source': 'TBL_A', 'right_source': 'TBL_C'}, transformed_data = DataFrame[c1: bigint, c2: bigint, c3: bigint, c4: bigint]
data_dict = {'TBL_A': DataFrame[c1: bigint, c2: bigint, c3: bigint, c4: bigint], 'TBL_B': DataFrame[c1: bigint, c5: string, c7: st...'TBL_C': DataFrame[c1c: bigint, c6: string, c8: string], 'TBL_C2': DataFrame[c2c: bigint, c6: string, c8: string], ...}

    def _apply_transformation(
        self,
        tf_step: str,
        tf_params: dict,
        transformed_data: DataFrame,
        data_dict: dict[str, DataFrame],
    ) -> DataFrame:
        """Apply a specific transformation step to the data.

        Args:
            tf_step (str): Type of transformation (join, add_variables, etc.)
            tf_params (dict): Parameters for the transformation
            transformed_data (DataFrame): Current state of the data
            data_dict (dict[str, DataFrame]): Dictionary of all available data sources

        Returns:
            DataFrame: Data after applying the transformation
        """
        # Get required arguments for the transformation
>       kwgs = get_required_arguments({tf_step: tf_params}, self, tf_step)
E       TypeError: unhashable type: 'dict'

src\extract\master_data_sql.py:103: TypeError
_________________________________________________________________________________ test_full_master_data_sql[right_sources3-output_data3] _________________________________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x0000026137717640>, right_sources = ['TBL_D_r', 'TBL_D_l', 'TBL_D'], output_data = [(1, 2, 3, 4, 1, 2, ...)]

    @pytest.mark.parametrize(
        ("right_sources", "output_data"),
        [
            # All tables are joined from `JOINS` list, so 13 columns in total
            (
                ["TBL_B", "TBL_C", "TBL_C2"],
                [
                    (1, 2, 3, 4, 1, "A", "B", 1, "E", "F", 1, "C", "D"),
                    (1, 2, 3, 4, 1, "A", "B", 1, "C", "D", 1, "C", "D"),
                ],
            ),
            # Subset of tables is joined from `JOINS` list, so 10 columns in total
            (
                ["TBL_C", "TBL_C2"],
                [
                    (1, 2, 3, 4, 1, "E", "F", 1, "C", "D"),
                    (1, 2, 3, 4, 1, "C", "D", 1, "C", "D"),
                ],
            ),
            # One to one mapping, no transformations
            (
                ["TBL_A"],
                [
                    (1, 2, 3, 4),
                ],
            ),
            # Checking CASE statements
            (
                ["TBL_D_r", "TBL_D_l", "TBL_D"],
                [
                    (1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4),
                ],
            ),
        ],
    )
    def test_full_master_data_sql(spark_session, right_sources, output_data):
        """Test `get_full_master_data_sql`.

        In this test, all sources are loaded in temporary view.
        The total number of columns of the output dataset should match

        The number of columns specified in `SOURCES` & joined according
        to the configuration in `JOINS`.
        """
        for s in SOURCES:
            spark_session.createDataFrame(
                s["data"], schema=s["columns"]
            ).createOrReplaceTempView(s["source"])

        business_logic_dict = {
            "sources": SOURCES,
            "transformations": [
                {"join": j} for j in JOINS if j["right_source"] in right_sources
            ],
        }

        if len(business_logic_dict["transformations"]) == 0:
            business_logic_dict.pop("transformations", None)
        else:
            business_logic_dict["transformations"][0]["join"] = {
                "left_source": "TBL_A",
                **business_logic_dict["transformations"][0]["join"],
            }

        result = GetIntegratedData(
            spark_session,
            business_logic=business_logic_dict,
>       ).get_integrated_data()

test\test_extract\test_get_master_data.py:158: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  
src\extract\master_data_sql.py:34: in get_integrated_data
    return self.transform_data(data_dict=data_dict)
src\extract\master_data_sql.py:73: in transform_data
    transformed_data = self._apply_transformation(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  

self = <src.extract.master_data_sql.GetIntegratedData object at 0x0000026136DCB7C0>
tf_step = ('join', {'condition': ['TBL_A.c1 = CASE WHEN TBL_D_r.d1 = 1 THEN 1 END'], 'how': 'left', 'left_source': 'TBL_A', 'right_source': 'TBL_D_r'})
tf_params = {'condition': ['TBL_A.c1 = CASE WHEN TBL_D_r.d1 = 1 THEN 1 END'], 'how': 'left', 'left_source': 'TBL_A', 'right_source': 'TBL_D_r'}
transformed_data = DataFrame[c1: bigint, c2: bigint, c3: bigint, c4: bigint]
data_dict = {'TBL_A': DataFrame[c1: bigint, c2: bigint, c3: bigint, c4: bigint], 'TBL_B': DataFrame[c1: bigint, c5: string, c7: st...'TBL_C': DataFrame[c1c: bigint, c6: string, c8: string], 'TBL_C2': DataFrame[c2c: bigint, c6: string, c8: string], ...}

    def _apply_transformation(
        self,
        tf_step: str,
        tf_params: dict,
        transformed_data: DataFrame,
        data_dict: dict[str, DataFrame],
    ) -> DataFrame:
        """Apply a specific transformation step to the data.

        Args:
            tf_step (str): Type of transformation (join, add_variables, etc.)
            tf_params (dict): Parameters for the transformation
            transformed_data (DataFrame): Current state of the data
            data_dict (dict[str, DataFrame]): Dictionary of all available data sources

        Returns:
            DataFrame: Data after applying the transformation
        """
        # Get required arguments for the transformation
>       kwgs = get_required_arguments({tf_step: tf_params}, self, tf_step)
E       TypeError: unhashable type: 'dict'

src\extract\master_data_sql.py:103: TypeError

---------- coverage: platform win32, python 3.10.11-final-0 ----------
Name                                            Stmts   Miss  Cover   Missing
-----------------------------------------------------------------------------
src\__init__.py                                     0      0   100%
src\config\__init__.py                              0      0   100%
src\config\constants.py                             1      0   100%
src\config\schema.py                                4      2    50%   51-52
src\dq\__init__.py                                  0      0   100%
src\dq\dq_validation.py                           141    141     0%   13-486
src\extract\__init__.py                             0      0   100%
src\extract\master_data_sql.py                    104     51    51%   80-82, 105-124, 135-137, 147-149, 173, 285-306, 332, 334-347, 349
src\month_setup\__init__.py                         0      0   100%
src\month_setup\dial_derive_snapshotdate.py        32     32     0%   8-85
src\month_setup\metadata_log_tables.py              1      1     0%   18
src\month_setup\setup_new_month.py                  1      1     0%   9
src\transform\__init__.py                           0      0   100%
src\transform\table_write_and_comment.py           58     58     0%   7-199
src\transform\transform_business_logic_sql.py       6      6     0%   5-24
src\utils\__init__.py                               0      0   100%
src\utils\alias_util.py                            13      6    54%   14-19, 101-109
src\utils\export_parquet.py                        14     14     0%   9-51
src\utils\get_catalog.py                            5      5     0%   6-20
src\utils\get_dbutils.py                            0      0   100%
src\utils\logging_util.py                           6      0   100%
src\utils\parameter_utils.py                       26     26     0%   6-124
src\utils\parse_yaml.py                            12     12     0%   9-31
src\utils\process_logging.py                       13     13     0%   9-60
src\utils\sources_util.py                          38     24    37%   40, 57, 72, 86, 99-103, 113, 135-138, 146-163, 169, 172, 176-178
src\utils\table_schema.py                           3      3     0%   8-16
src\validate\__init__.py                            0      0   100%
src\validate\base.py                                4      4     0%   4-7
src\validate\expressions.py                        24     24     0%   13-64
src\validate\run_all.py                             7      7     0%   11-44
src\validate\sources.py                            29     29     0%   7-67
src\validate\transformations.py                   170    170     0%   16-469
src\validate\validate_sql.py                       50     50     0%   12-115
src\validate\yaml.py                               18     18     0%   3-33
-----------------------------------------------------------------------------
TOTAL                                             780    697    11%
Coverage HTML written to dir htmlcov

================================================================================================ short test summary info ================================================================================================= 
FAILED test/test_extract/test_get_master_data.py::test_full_master_data_sql[right_sources0-output_data0] - TypeError: unhashable type: 'dict'
FAILED test/test_extract/test_get_master_data.py::test_full_master_data_sql[right_sources1-output_data1] - TypeError: unhashable type: 'dict'
FAILED test/test_extract/test_get_master_data.py::test_full_master_data_sql[right_sources3-output_data3] - TypeError: unhashable type: 'dict'
============================================================================================== 3 failed, 5 passed in 40.24s ============================================================================================== 
(bsrc-etl-v) PS C:\Users\B25712\bsrc-etl-venv\bsrc-etl> SUCCESS: The process with PID 6536 (child process of PID 12728) has been terminated.
SUCCESS: The process with PID 12728 (child process of PID 19136) has been terminated.
SUCCESS: The process with PID 19136 (child process of PID 25056) has been terminated.