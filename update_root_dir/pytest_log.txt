(bsrc-etl-venv) PS C:\Users\B25712\bsrc-etl-venv\bsrc-etl> pytest
===================================================================================== test session starts ======================================================================================
platform win32 -- Python 3.10.11, pytest-8.3.3, pluggy-1.5.0
rootdir: C:\Users\B25712\bsrc-etl-venv\bsrc-etl
configfile: pyproject.toml
plugins: cov-6.0.0, mock-3.14.0
collected 397 items

test\month_setup\test_dial_snapshotdate.py ............                                                                                                                                   [  3%]
test\scripts\test_check_dependencies.py ......                                                                                                                                            [  4%]
test\scripts\test_dial_check_delayed_files.py .....                                                                                                                                       [  5%]
test\scripts\test_dial_staging_process.py .................                                                                                                                               [ 10%]
test\scripts\test_export_tine_tables.py ....                                                                                                                                              [ 11%]
test\scripts\test_new_month_catalog_setup.py ...                                                                                                                                          [ 11%]
test\scripts\test_nonssf_staging_process.py .................                                                                                                                             [ 16%]
test\scripts\test_run_mapping.py ....................................                                                                                                                     [ 25%]
test\scripts\test_ssf_staging_process.py ....................                                                                                                                             [ 30%]
test\scripts\test_ssf_staging_process_xml.py ...                                                                                                                                          [ 30%]
test\staging\test_extract_dial_data.py .                                                                                                                                                  [ 31%]
test\staging\test_extract_non_ssf_data.py ....                                                                                                                                            [ 32%]
test\staging\test_extract_ssf_data.py ..........                                                                                                                                          [ 34%]
test\test_dq\test_dq_validation.py FF.FFFFFFFFFFF.FFFF.FFF.FF.FFF.FFFF                                                                                                                    [ 43%]
test\test_extract\test_get_master_data.py ..............                                                                                                                                  [ 47%]
test\test_transform\test_all_columns.py ....                                                                                                                                              [ 48%]
test\test_transform\test_complex_types.py ...                                                                                                                                             [ 48%]
test\test_transform\test_expressions.py .....                                                                                                                                             [ 50%]
test\test_transform\test_pipeline_yaml_integrated_target.py F.FF                                                                                                                          [ 51%]
test\test_transform\test_transform_business_logic_sql.py ..........                                                                                                                       [ 53%]
test\test_transform\test_validate.py ..........................................................                                                                                           [ 68%]
test\test_transform\test_write_and_comment.py ............................                                                                                                                [ 75%]
test\test_utils\test_azure_utils.py .                                                                                                                                                     [ 75%]
test\test_utils\test_export_parquet.py ..                                                                                                                                                 [ 76%]
test\test_utils\test_get_catalog.py .....                                                                                                                                                 [ 77%]
test\test_utils\test_metadata_log_tables.py ...                                                                                                                                           [ 78%]
test\test_utils\test_parameter_utils.py ...........................................                                                                                                       [ 88%]
test\test_utils\test_parse_yaml.py F                                                                                                                                                      [ 89%]
test\test_utils\test_process_logging.py ........                                                                                                                                          [ 91%]
test\test_utils\test_sources_util.py ......                                                                                                                                               [ 92%]
test\test_utils\test_table_schema.py F                                                                                                                                                    [ 92%]
test\test_utils\test_xml_utils.py ............................                                                                                                                            [100%]

=========================================================================================== FAILURES =========================================================================================== 
_____________________________________________________________ test_dq_validation[dq_test_happy-True-Checks completed successfully] _____________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, validation_output = True, reference_data = None, expected_logging = 'Checks completed successfully'
main_data = ('dq_test_happy', None), caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99EBF6D0>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            ("dq_test_happy", True, "Checks completed successfully"),
            (
                "dq_test_happy_col_num",
                True,
                "Checks completed successfully",
            ),
            ("dq_test_no_checks", None, "No checks done"),
            (
                "dq_test_unhappy_col_num",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_pk_dup",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_pk_null",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_not_null",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_num_cols",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_type_cols",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_ref",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_ref_filter",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_unique",
                False,
                "Checks completed - DQ issues found",
            ),
        ],
        indirect=["main_data"],
    )
    def test_dq_validation(
        spark_session,
        validation_output,
        reference_data,
        expected_logging,
        main_data,
        caplog,
    ):
        """Test full DQ validation for a given table"""

        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            run_month="",
            dq_check_folder="test/data",
            local=True,
        )

>       assert dq_validation.checks(functional=True) == validation_output
E       assert None == True
E        +  where None = checks(functional=True)
E        +    where checks = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99EBF730>.checks

test\test_dq\test_dq_validation.py:128: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:08 [INFO] __init__:  No specific checks file available for dq.dq_test_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy.yml
2025-09-10 16:00:08 [INFO] __init__:  No specific checks file available for dq.dq_test_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy.yml
2025-09-10 16:00:08 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:08 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:08 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:08 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:08 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:08 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:08 [INFO] unique:  No unique columns to check
2025-09-10 16:00:08 [INFO] unique:  No unique columns to check
2025-09-10 16:00:08 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:08 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:08 [INFO] checks:  No checks done
2025-09-10 16:00:08 [INFO] checks:  No checks done
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy.yml       
INFO     betl_src_poc_logger:dq_validation.py:210 No columns to check
INFO     betl_src_poc_logger:dq_validation.py:278 No Primary Key to check
INFO     betl_src_poc_logger:dq_validation.py:387 No not nullable columns to check
INFO     betl_src_poc_logger:dq_validation.py:333 No unique columns to check
INFO     betl_src_poc_logger:dq_validation.py:427 No referential integrity checks
INFO     betl_src_poc_logger:dq_validation.py:171 No checks done
_________________________________________________________ test_dq_validation[dq_test_happy_col_num-True-Checks completed successfully] _________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, validation_output = True, reference_data = None, expected_logging = 'Checks completed successfully'
main_data = ('dq_test_happy_col_num', None), caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99E774F0>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            ("dq_test_happy", True, "Checks completed successfully"),
            (
                "dq_test_happy_col_num",
                True,
                "Checks completed successfully",
            ),
            ("dq_test_no_checks", None, "No checks done"),
            (
                "dq_test_unhappy_col_num",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_pk_dup",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_pk_null",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_not_null",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_num_cols",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_type_cols",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_ref",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_ref_filter",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_unique",
                False,
                "Checks completed - DQ issues found",
            ),
        ],
        indirect=["main_data"],
    )
    def test_dq_validation(
        spark_session,
        validation_output,
        reference_data,
        expected_logging,
        main_data,
        caplog,
    ):
        """Test full DQ validation for a given table"""

        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            run_month="",
            dq_check_folder="test/data",
            local=True,
        )

>       assert dq_validation.checks(functional=True) == validation_output
E       assert None == True
E        +  where None = checks(functional=True)
E        +    where checks = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99E77400>.checks

test\test_dq\test_dq_validation.py:128: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:08 [INFO] __init__:  No specific checks file available for dq.dq_test_happy_col_num at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy_col_num.yml  
2025-09-10 16:00:08 [INFO] __init__:  No specific checks file available for dq.dq_test_happy_col_num at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy_col_num.yml  
2025-09-10 16:00:08 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:08 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:08 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:08 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:08 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:08 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:08 [INFO] unique:  No unique columns to check
2025-09-10 16:00:08 [INFO] unique:  No unique columns to check
2025-09-10 16:00:08 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:08 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:08 [INFO] checks:  No checks done
2025-09-10 16:00:08 [INFO] checks:  No checks done
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_happy_col_num at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy_col_num.yml
INFO     betl_src_poc_logger:dq_validation.py:210 No columns to check
INFO     betl_src_poc_logger:dq_validation.py:278 No Primary Key to check
INFO     betl_src_poc_logger:dq_validation.py:387 No not nullable columns to check
INFO     betl_src_poc_logger:dq_validation.py:333 No unique columns to check
INFO     betl_src_poc_logger:dq_validation.py:427 No referential integrity checks
INFO     betl_src_poc_logger:dq_validation.py:171 No checks done
_____________________________________________________ test_dq_validation[dq_test_unhappy_col_num-False-Checks completed - DQ issues found] _____________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, validation_output = False, reference_data = None, expected_logging = 'Checks completed - DQ issues found'
main_data = ('dq_test_unhappy_col_num', None), caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99FFADD0>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            ("dq_test_happy", True, "Checks completed successfully"),
            (
                "dq_test_happy_col_num",
                True,
                "Checks completed successfully",
            ),
            ("dq_test_no_checks", None, "No checks done"),
            (
                "dq_test_unhappy_col_num",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_pk_dup",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_pk_null",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_not_null",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_num_cols",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_type_cols",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_ref",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_ref_filter",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_unique",
                False,
                "Checks completed - DQ issues found",
            ),
        ],
        indirect=["main_data"],
    )
    def test_dq_validation(
        spark_session,
        validation_output,
        reference_data,
        expected_logging,
        main_data,
        caplog,
    ):
        """Test full DQ validation for a given table"""

        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            run_month="",
            dq_check_folder="test/data",
            local=True,
        )

>       assert dq_validation.checks(functional=True) == validation_output
E       assert None == False
E        +  where None = checks(functional=True)
E        +    where checks = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99FA2AD0>.checks

test\test_dq\test_dq_validation.py:128: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:08 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_col_num at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_col_num.yml
2025-09-10 16:00:08 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_col_num at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_col_num.yml
2025-09-10 16:00:08 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:08 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:08 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:08 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:08 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:08 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:08 [INFO] unique:  No unique columns to check
2025-09-10 16:00:08 [INFO] unique:  No unique columns to check
2025-09-10 16:00:08 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:08 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:08 [INFO] checks:  No checks done
2025-09-10 16:00:08 [INFO] checks:  No checks done
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_unhappy_col_num at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_col_num.yml
INFO     betl_src_poc_logger:dq_validation.py:210 No columns to check
INFO     betl_src_poc_logger:dq_validation.py:278 No Primary Key to check
INFO     betl_src_poc_logger:dq_validation.py:387 No not nullable columns to check
INFO     betl_src_poc_logger:dq_validation.py:333 No unique columns to check
INFO     betl_src_poc_logger:dq_validation.py:427 No referential integrity checks
INFO     betl_src_poc_logger:dq_validation.py:171 No checks done
_____________________________________________________ test_dq_validation[dq_test_unhappy_pk_dup-False-Checks completed - DQ issues found] ______________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, validation_output = False, reference_data = None, expected_logging = 'Checks completed - DQ issues found'       
main_data = ('dq_test_unhappy_pk_dup', None), caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99E74F40>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            ("dq_test_happy", True, "Checks completed successfully"),
            (
                "dq_test_happy_col_num",
                True,
                "Checks completed successfully",
            ),
            ("dq_test_no_checks", None, "No checks done"),
            (
                "dq_test_unhappy_col_num",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_pk_dup",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_pk_null",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_not_null",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_num_cols",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_type_cols",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_ref",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_ref_filter",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_unique",
                False,
                "Checks completed - DQ issues found",
            ),
        ],
        indirect=["main_data"],
    )
    def test_dq_validation(
        spark_session,
        validation_output,
        reference_data,
        expected_logging,
        main_data,
        caplog,
    ):
        """Test full DQ validation for a given table"""

        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            run_month="",
            dq_check_folder="test/data",
            local=True,
        )

>       assert dq_validation.checks(functional=True) == validation_output
E       assert None == False
E        +  where None = checks(functional=True)
E        +    where checks = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99E74610>.checks

test\test_dq\test_dq_validation.py:128: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:09 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_pk_dup at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_pk_dup.yml
2025-09-10 16:00:09 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_pk_dup at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_pk_dup.yml
2025-09-10 16:00:09 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:09 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:09 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:09 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:09 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:09 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:09 [INFO] unique:  No unique columns to check
2025-09-10 16:00:09 [INFO] unique:  No unique columns to check
2025-09-10 16:00:09 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:09 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:09 [INFO] checks:  No checks done
2025-09-10 16:00:09 [INFO] checks:  No checks done
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_unhappy_pk_dup at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_pk_dup.yml
INFO     betl_src_poc_logger:dq_validation.py:210 No columns to check
INFO     betl_src_poc_logger:dq_validation.py:278 No Primary Key to check
INFO     betl_src_poc_logger:dq_validation.py:387 No not nullable columns to check
INFO     betl_src_poc_logger:dq_validation.py:333 No unique columns to check
INFO     betl_src_poc_logger:dq_validation.py:427 No referential integrity checks
INFO     betl_src_poc_logger:dq_validation.py:171 No checks done
_____________________________________________________ test_dq_validation[dq_test_unhappy_pk_null-False-Checks completed - DQ issues found] _____________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, validation_output = False, reference_data = None, expected_logging = 'Checks completed - DQ issues found'       
main_data = ('dq_test_unhappy_pk_null', None), caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99FFA6B0>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            ("dq_test_happy", True, "Checks completed successfully"),
            (
                "dq_test_happy_col_num",
                True,
                "Checks completed successfully",
            ),
            ("dq_test_no_checks", None, "No checks done"),
            (
                "dq_test_unhappy_col_num",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_pk_dup",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_pk_null",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_not_null",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_num_cols",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_type_cols",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_ref",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_ref_filter",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_unique",
                False,
                "Checks completed - DQ issues found",
            ),
        ],
        indirect=["main_data"],
    )
    def test_dq_validation(
        spark_session,
        validation_output,
        reference_data,
        expected_logging,
        main_data,
        caplog,
    ):
        """Test full DQ validation for a given table"""

        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            run_month="",
            dq_check_folder="test/data",
            local=True,
        )

>       assert dq_validation.checks(functional=True) == validation_output
E       assert None == False
E        +  where None = checks(functional=True)
E        +    where checks = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99FF83D0>.checks

test\test_dq\test_dq_validation.py:128: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:09 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_pk_null at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_pk_null.yml
2025-09-10 16:00:09 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_pk_null at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_pk_null.yml
2025-09-10 16:00:09 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:09 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:09 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:09 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:09 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:09 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:09 [INFO] unique:  No unique columns to check
2025-09-10 16:00:09 [INFO] unique:  No unique columns to check
2025-09-10 16:00:09 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:09 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:09 [INFO] checks:  No checks done
2025-09-10 16:00:09 [INFO] checks:  No checks done
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_unhappy_pk_null at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_pk_null.yml
INFO     betl_src_poc_logger:dq_validation.py:210 No columns to check
INFO     betl_src_poc_logger:dq_validation.py:278 No Primary Key to check
INFO     betl_src_poc_logger:dq_validation.py:387 No not nullable columns to check
INFO     betl_src_poc_logger:dq_validation.py:333 No unique columns to check
INFO     betl_src_poc_logger:dq_validation.py:427 No referential integrity checks
INFO     betl_src_poc_logger:dq_validation.py:171 No checks done
____________________________________________________ test_dq_validation[dq_test_unhappy_not_null-False-Checks completed - DQ issues found] _____________________________________________________

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, validation_output = False, reference_data = None, expected_logging = 'Checks completed - DQ issues found'       
main_data = ('dq_test_unhappy_not_null', None), caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99CB0BB0>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            ("dq_test_happy", True, "Checks completed successfully"),
            (
                "dq_test_happy_col_num",
                True,
                "Checks completed successfully",
            ),
            ("dq_test_no_checks", None, "No checks done"),
            (
                "dq_test_unhappy_col_num",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_pk_dup",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_pk_null",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_not_null",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_num_cols",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_type_cols",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_ref",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_ref_filter",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_unique",
                False,
                "Checks completed - DQ issues found",
            ),
        ],
        indirect=["main_data"],
    )
    def test_dq_validation(
        spark_session,
        validation_output,
        reference_data,
        expected_logging,
        main_data,
        caplog,
    ):
        """Test full DQ validation for a given table"""

        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            run_month="",
            dq_check_folder="test/data",
            local=True,
        )

>       assert dq_validation.checks(functional=True) == validation_output
E       assert None == False
E        +  where None = checks(functional=True)
E        +    where checks = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99CB2E60>.checks

test\test_dq\test_dq_validation.py:128: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:09 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_not_null at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_not_null.yml
2025-09-10 16:00:09 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_not_null at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_not_null.yml
2025-09-10 16:00:09 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:09 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:09 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:09 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:09 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:09 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:09 [INFO] unique:  No unique columns to check
2025-09-10 16:00:09 [INFO] unique:  No unique columns to check
2025-09-10 16:00:09 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:09 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:09 [INFO] checks:  No checks done
2025-09-10 16:00:09 [INFO] checks:  No checks done
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_unhappy_not_null at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_not_null.yml
INFO     betl_src_poc_logger:dq_validation.py:210 No columns to check
INFO     betl_src_poc_logger:dq_validation.py:278 No Primary Key to check
INFO     betl_src_poc_logger:dq_validation.py:387 No not nullable columns to check
INFO     betl_src_poc_logger:dq_validation.py:333 No unique columns to check
INFO     betl_src_poc_logger:dq_validation.py:427 No referential integrity checks
INFO     betl_src_poc_logger:dq_validation.py:171 No checks done
____________________________________________________ test_dq_validation[dq_test_unhappy_num_cols-False-Checks completed - DQ issues found] _____________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, validation_output = False, reference_data = None, expected_logging = 'Checks completed - DQ issues found'       
main_data = ('dq_test_unhappy_num_cols', None), caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99FF8C40>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            ("dq_test_happy", True, "Checks completed successfully"),
            (
                "dq_test_happy_col_num",
                True,
                "Checks completed successfully",
            ),
            ("dq_test_no_checks", None, "No checks done"),
            (
                "dq_test_unhappy_col_num",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_pk_dup",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_pk_null",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_not_null",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_num_cols",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_type_cols",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_ref",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_ref_filter",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_unique",
                False,
                "Checks completed - DQ issues found",
            ),
        ],
        indirect=["main_data"],
    )
    def test_dq_validation(
        spark_session,
        validation_output,
        reference_data,
        expected_logging,
        main_data,
        caplog,
    ):
        """Test full DQ validation for a given table"""

        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            run_month="",
            dq_check_folder="test/data",
            local=True,
        )

>       assert dq_validation.checks(functional=True) == validation_output
E       assert None == False
E        +  where None = checks(functional=True)
E        +    where checks = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99FF9900>.checks

test\test_dq\test_dq_validation.py:128: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:09 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_num_cols at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_num_cols.yml
2025-09-10 16:00:09 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_num_cols at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_num_cols.yml
2025-09-10 16:00:09 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:09 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:09 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:09 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:09 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:09 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:09 [INFO] unique:  No unique columns to check
2025-09-10 16:00:09 [INFO] unique:  No unique columns to check
2025-09-10 16:00:09 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:09 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:09 [INFO] checks:  No checks done
2025-09-10 16:00:09 [INFO] checks:  No checks done
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_unhappy_num_cols at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_num_cols.yml
INFO     betl_src_poc_logger:dq_validation.py:210 No columns to check
INFO     betl_src_poc_logger:dq_validation.py:278 No Primary Key to check
INFO     betl_src_poc_logger:dq_validation.py:387 No not nullable columns to check
INFO     betl_src_poc_logger:dq_validation.py:333 No unique columns to check
INFO     betl_src_poc_logger:dq_validation.py:427 No referential integrity checks
INFO     betl_src_poc_logger:dq_validation.py:171 No checks done
____________________________________________________ test_dq_validation[dq_test_unhappy_type_cols-False-Checks completed - DQ issues found] ____________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, validation_output = False, reference_data = None, expected_logging = 'Checks completed - DQ issues found'       
main_data = ('dq_test_unhappy_type_cols', None), caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99CB3370>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            ("dq_test_happy", True, "Checks completed successfully"),
            (
                "dq_test_happy_col_num",
                True,
                "Checks completed successfully",
            ),
            ("dq_test_no_checks", None, "No checks done"),
            (
                "dq_test_unhappy_col_num",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_pk_dup",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_pk_null",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_not_null",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_num_cols",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_type_cols",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_ref",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_ref_filter",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_unique",
                False,
                "Checks completed - DQ issues found",
            ),
        ],
        indirect=["main_data"],
    )
    def test_dq_validation(
        spark_session,
        validation_output,
        reference_data,
        expected_logging,
        main_data,
        caplog,
    ):
        """Test full DQ validation for a given table"""

        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            run_month="",
            dq_check_folder="test/data",
            local=True,
        )

>       assert dq_validation.checks(functional=True) == validation_output
E       assert None == False
E        +  where None = checks(functional=True)
E        +    where checks = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99CB1D20>.checks

test\test_dq\test_dq_validation.py:128: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:09 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_type_cols at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_type_cols.yml
2025-09-10 16:00:09 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_type_cols at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_type_cols.yml
2025-09-10 16:00:09 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:09 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:09 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:09 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:09 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:09 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:09 [INFO] unique:  No unique columns to check
2025-09-10 16:00:09 [INFO] unique:  No unique columns to check
2025-09-10 16:00:09 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:09 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:09 [INFO] checks:  No checks done
2025-09-10 16:00:09 [INFO] checks:  No checks done
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_unhappy_type_cols at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_type_cols.yml
INFO     betl_src_poc_logger:dq_validation.py:210 No columns to check
INFO     betl_src_poc_logger:dq_validation.py:278 No Primary Key to check
INFO     betl_src_poc_logger:dq_validation.py:387 No not nullable columns to check
INFO     betl_src_poc_logger:dq_validation.py:333 No unique columns to check
INFO     betl_src_poc_logger:dq_validation.py:427 No referential integrity checks
INFO     betl_src_poc_logger:dq_validation.py:171 No checks done
_______________________________________________________ test_dq_validation[dq_test_unhappy_ref-False-Checks completed - DQ issues found] _______________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, validation_output = False, reference_data = None, expected_logging = 'Checks completed - DQ issues found'       
main_data = ('dq_test_unhappy_ref', None), caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC9A046230>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            ("dq_test_happy", True, "Checks completed successfully"),
            (
                "dq_test_happy_col_num",
                True,
                "Checks completed successfully",
            ),
            ("dq_test_no_checks", None, "No checks done"),
            (
                "dq_test_unhappy_col_num",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_pk_dup",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_pk_null",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_not_null",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_num_cols",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_type_cols",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_ref",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_ref_filter",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_unique",
                False,
                "Checks completed - DQ issues found",
            ),
        ],
        indirect=["main_data"],
    )
    def test_dq_validation(
        spark_session,
        validation_output,
        reference_data,
        expected_logging,
        main_data,
        caplog,
    ):
        """Test full DQ validation for a given table"""

        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            run_month="",
            dq_check_folder="test/data",
            local=True,
        )

>       assert dq_validation.checks(functional=True) == validation_output
E       assert None == False
E        +  where None = checks(functional=True)
E        +    where checks = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC9A044250>.checks

test\test_dq\test_dq_validation.py:128: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:09 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_ref at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_ref.yml      
2025-09-10 16:00:09 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_ref at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_ref.yml      
2025-09-10 16:00:09 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:09 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:09 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:09 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:09 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:09 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:09 [INFO] unique:  No unique columns to check
2025-09-10 16:00:09 [INFO] unique:  No unique columns to check
2025-09-10 16:00:09 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:09 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:09 [INFO] checks:  No checks done
2025-09-10 16:00:09 [INFO] checks:  No checks done
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_unhappy_ref at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_ref.yml
INFO     betl_src_poc_logger:dq_validation.py:210 No columns to check
INFO     betl_src_poc_logger:dq_validation.py:278 No Primary Key to check
INFO     betl_src_poc_logger:dq_validation.py:387 No not nullable columns to check
INFO     betl_src_poc_logger:dq_validation.py:333 No unique columns to check
INFO     betl_src_poc_logger:dq_validation.py:427 No referential integrity checks
INFO     betl_src_poc_logger:dq_validation.py:171 No checks done
___________________________________________________ test_dq_validation[dq_test_unhappy_ref_filter-False-Checks completed - DQ issues found] ____________________________________________________

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, validation_output = False, reference_data = None, expected_logging = 'Checks completed - DQ issues found'       
main_data = ('dq_test_unhappy_ref_filter', None), caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99CB1360>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            ("dq_test_happy", True, "Checks completed successfully"),
            (
                "dq_test_happy_col_num",
                True,
                "Checks completed successfully",
            ),
            ("dq_test_no_checks", None, "No checks done"),
            (
                "dq_test_unhappy_col_num",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_pk_dup",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_pk_null",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_not_null",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_num_cols",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_type_cols",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_ref",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_ref_filter",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_unique",
                False,
                "Checks completed - DQ issues found",
            ),
        ],
        indirect=["main_data"],
    )
    def test_dq_validation(
        spark_session,
        validation_output,
        reference_data,
        expected_logging,
        main_data,
        caplog,
    ):
        """Test full DQ validation for a given table"""

        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            run_month="",
            dq_check_folder="test/data",
            local=True,
        )

>       assert dq_validation.checks(functional=True) == validation_output
E       assert None == False
E        +  where None = checks(functional=True)
E        +    where checks = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99CB0400>.checks

test\test_dq\test_dq_validation.py:128: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:09 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_ref_filter at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_ref_filter.yml
2025-09-10 16:00:09 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_ref_filter at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_ref_filter.yml
2025-09-10 16:00:09 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:09 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:09 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:09 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:09 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:09 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:09 [INFO] unique:  No unique columns to check
2025-09-10 16:00:09 [INFO] unique:  No unique columns to check
2025-09-10 16:00:09 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:09 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:09 [INFO] checks:  No checks done
2025-09-10 16:00:09 [INFO] checks:  No checks done
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_unhappy_ref_filter at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_ref_filter.yml
INFO     betl_src_poc_logger:dq_validation.py:210 No columns to check
INFO     betl_src_poc_logger:dq_validation.py:278 No Primary Key to check
INFO     betl_src_poc_logger:dq_validation.py:387 No not nullable columns to check
INFO     betl_src_poc_logger:dq_validation.py:333 No unique columns to check
INFO     betl_src_poc_logger:dq_validation.py:427 No referential integrity checks
INFO     betl_src_poc_logger:dq_validation.py:171 No checks done
_____________________________________________________ test_dq_validation[dq_test_unhappy_unique-False-Checks completed - DQ issues found] ______________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, validation_output = False, reference_data = None, expected_logging = 'Checks completed - DQ issues found'       
main_data = ('dq_test_unhappy_unique', None), caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC9A0441C0>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            ("dq_test_happy", True, "Checks completed successfully"),
            (
                "dq_test_happy_col_num",
                True,
                "Checks completed successfully",
            ),
            ("dq_test_no_checks", None, "No checks done"),
            (
                "dq_test_unhappy_col_num",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_pk_dup",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_pk_null",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_not_null",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_num_cols",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_type_cols",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_ref",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_ref_filter",
                False,
                "Checks completed - DQ issues found",
            ),
            (
                "dq_test_unhappy_unique",
                False,
                "Checks completed - DQ issues found",
            ),
        ],
        indirect=["main_data"],
    )
    def test_dq_validation(
        spark_session,
        validation_output,
        reference_data,
        expected_logging,
        main_data,
        caplog,
    ):
        """Test full DQ validation for a given table"""

        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            run_month="",
            dq_check_folder="test/data",
            local=True,
        )

>       assert dq_validation.checks(functional=True) == validation_output
E       assert None == False
E        +  where None = checks(functional=True)
E        +    where checks = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC9A044070>.checks

test\test_dq\test_dq_validation.py:128: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:09 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_unique at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_unique.yml
2025-09-10 16:00:09 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_unique at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_unique.yml
2025-09-10 16:00:09 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:09 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:09 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:09 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:09 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:09 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:09 [INFO] unique:  No unique columns to check
2025-09-10 16:00:09 [INFO] unique:  No unique columns to check
2025-09-10 16:00:09 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:09 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:09 [INFO] checks:  No checks done
2025-09-10 16:00:09 [INFO] checks:  No checks done
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_unhappy_unique at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_unique.yml
INFO     betl_src_poc_logger:dq_validation.py:210 No columns to check
INFO     betl_src_poc_logger:dq_validation.py:278 No Primary Key to check
INFO     betl_src_poc_logger:dq_validation.py:387 No not nullable columns to check
INFO     betl_src_poc_logger:dq_validation.py:333 No unique columns to check
INFO     betl_src_poc_logger:dq_validation.py:427 No referential integrity checks
INFO     betl_src_poc_logger:dq_validation.py:171 No checks done
______________________________________________________________________ test_columns[dq_test_happy-True-expected_logging0] ______________________________________________________________________

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, main_data = ('dq_test_happy', None), reference_data = None, validation_output = True
expected_logging = ['Number of columns matches: 4 expected and received', 'Datatypes match'], caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99CB13C0>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            (
                "dq_test_happy",
                True,
                ["Number of columns matches: 4 expected and received", "Datatypes match"],
            ),
            (
                "dq_test_happy_col_num",
                True,
                [
                    "Number of columns matches: 4 expected and received",
                    "No datatypes checked",
                ],
            ),
            (
                "dq_test_no_checks",
                None,
                ["No columns to check"],
            ),
            (
                "dq_test_unhappy_col_num",
                False,
                [
                    "Number of columns incorrect: expected 5, received 4",
                    "No datatypes checked",
                ],
            ),
            (
                "dq_test_unhappy_num_cols",
                False,
                [
                    "Number of columns incorrect: expected 5, received 4",
                    "Mismatch in datatypes, differences: "
                    "expected [('Extra', 'int')], received []",
                ],
            ),
            (
                "dq_test_unhappy_type_cols",
                False,
                [
                    "Number of columns matches: 4 expected and received",
                    "Mismatch in datatypes, differences: "
                    "expected [('Type', 'int')], received [('Type', 'string')]",
                ],
            ),
        ],
        indirect=["main_data"],
    )
    def test_columns(
        spark_session,
        main_data,
        reference_data,
        validation_output,
        expected_logging,
        caplog,
    ):
        """Test number of columns and datatypes for a given table"""
        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            dq_check_folder="test/data",
            schema_name="dq",
            run_month="",
            local=True,
        )

>       assert dq_validation.columns_datatypes().get("result", None) == validation_output
E       AssertionError: assert None == True
E        +  where None = <built-in method get of dict object at 0x000001AC99D8FE40>('result', None)
E        +    where <built-in method get of dict object at 0x000001AC99D8FE40> = {}.get
E        +      where {} = columns_datatypes()
E        +        where columns_datatypes = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99DA7280>.columns_datatypes

test\test_dq\test_dq_validation.py:202: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:10 [INFO] __init__:  No specific checks file available for dq.dq_test_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy.yml
2025-09-10 16:00:10 [INFO] __init__:  No specific checks file available for dq.dq_test_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy.yml
2025-09-10 16:00:10 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:10 [INFO] columns_datatypes:  No columns to check
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy.yml       
INFO     betl_src_poc_logger:dq_validation.py:210 No columns to check
__________________________________________________________________ test_columns[dq_test_happy_col_num-True-expected_logging1] __________________________________________________________________

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, main_data = ('dq_test_happy_col_num', None), reference_data = None, validation_output = True
expected_logging = ['Number of columns matches: 4 expected and received', 'No datatypes checked'], caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99F671C0>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            (
                "dq_test_happy",
                True,
                ["Number of columns matches: 4 expected and received", "Datatypes match"],
            ),
            (
                "dq_test_happy_col_num",
                True,
                [
                    "Number of columns matches: 4 expected and received",
                    "No datatypes checked",
                ],
            ),
            (
                "dq_test_no_checks",
                None,
                ["No columns to check"],
            ),
            (
                "dq_test_unhappy_col_num",
                False,
                [
                    "Number of columns incorrect: expected 5, received 4",
                    "No datatypes checked",
                ],
            ),
            (
                "dq_test_unhappy_num_cols",
                False,
                [
                    "Number of columns incorrect: expected 5, received 4",
                    "Mismatch in datatypes, differences: "
                    "expected [('Extra', 'int')], received []",
                ],
            ),
            (
                "dq_test_unhappy_type_cols",
                False,
                [
                    "Number of columns matches: 4 expected and received",
                    "Mismatch in datatypes, differences: "
                    "expected [('Type', 'int')], received [('Type', 'string')]",
                ],
            ),
        ],
        indirect=["main_data"],
    )
    def test_columns(
        spark_session,
        main_data,
        reference_data,
        validation_output,
        expected_logging,
        caplog,
    ):
        """Test number of columns and datatypes for a given table"""
        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            dq_check_folder="test/data",
            schema_name="dq",
            run_month="",
            local=True,
        )

>       assert dq_validation.columns_datatypes().get("result", None) == validation_output
E       AssertionError: assert None == True
E        +  where None = <built-in method get of dict object at 0x000001AC9A066380>('result', None)
E        +    where <built-in method get of dict object at 0x000001AC9A066380> = {}.get
E        +      where {} = columns_datatypes()
E        +        where columns_datatypes = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99F673D0>.columns_datatypes

test\test_dq\test_dq_validation.py:202: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:10 [INFO] __init__:  No specific checks file available for dq.dq_test_happy_col_num at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy_col_num.yml  
2025-09-10 16:00:10 [INFO] __init__:  No specific checks file available for dq.dq_test_happy_col_num at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy_col_num.yml  
2025-09-10 16:00:10 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:10 [INFO] columns_datatypes:  No columns to check
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_happy_col_num at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy_col_num.yml
INFO     betl_src_poc_logger:dq_validation.py:210 No columns to check
________________________________________________________________ test_columns[dq_test_unhappy_col_num-False-expected_logging3] _________________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, main_data = ('dq_test_unhappy_col_num', None), reference_data = None, validation_output = False
expected_logging = ['Number of columns incorrect: expected 5, received 4', 'No datatypes checked'], caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99D001F0>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            (
                "dq_test_happy",
                True,
                ["Number of columns matches: 4 expected and received", "Datatypes match"],
            ),
            (
                "dq_test_happy_col_num",
                True,
                [
                    "Number of columns matches: 4 expected and received",
                    "No datatypes checked",
                ],
            ),
            (
                "dq_test_no_checks",
                None,
                ["No columns to check"],
            ),
            (
                "dq_test_unhappy_col_num",
                False,
                [
                    "Number of columns incorrect: expected 5, received 4",
                    "No datatypes checked",
                ],
            ),
            (
                "dq_test_unhappy_num_cols",
                False,
                [
                    "Number of columns incorrect: expected 5, received 4",
                    "Mismatch in datatypes, differences: "
                    "expected [('Extra', 'int')], received []",
                ],
            ),
            (
                "dq_test_unhappy_type_cols",
                False,
                [
                    "Number of columns matches: 4 expected and received",
                    "Mismatch in datatypes, differences: "
                    "expected [('Type', 'int')], received [('Type', 'string')]",
                ],
            ),
        ],
        indirect=["main_data"],
    )
    def test_columns(
        spark_session,
        main_data,
        reference_data,
        validation_output,
        expected_logging,
        caplog,
    ):
        """Test number of columns and datatypes for a given table"""
        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            dq_check_folder="test/data",
            schema_name="dq",
            run_month="",
            local=True,
        )

>       assert dq_validation.columns_datatypes().get("result", None) == validation_output
E       AssertionError: assert None == False
E        +  where None = <built-in method get of dict object at 0x000001AC99EC04C0>('result', None)
E        +    where <built-in method get of dict object at 0x000001AC99EC04C0> = {}.get
E        +      where {} = columns_datatypes()
E        +        where columns_datatypes = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99D03610>.columns_datatypes

test\test_dq\test_dq_validation.py:202: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:10 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_col_num at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_col_num.yml
2025-09-10 16:00:10 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_col_num at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_col_num.yml
2025-09-10 16:00:10 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:10 [INFO] columns_datatypes:  No columns to check
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_unhappy_col_num at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_col_num.yml
INFO     betl_src_poc_logger:dq_validation.py:210 No columns to check
________________________________________________________________ test_columns[dq_test_unhappy_num_cols-False-expected_logging4] ________________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, main_data = ('dq_test_unhappy_num_cols', None), reference_data = None, validation_output = False
expected_logging = ['Number of columns incorrect: expected 5, received 4', "Mismatch in datatypes, differences: expected [('Extra', 'int')], received []"]
caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99F67E50>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            (
                "dq_test_happy",
                True,
                ["Number of columns matches: 4 expected and received", "Datatypes match"],
            ),
            (
                "dq_test_happy_col_num",
                True,
                [
                    "Number of columns matches: 4 expected and received",
                    "No datatypes checked",
                ],
            ),
            (
                "dq_test_no_checks",
                None,
                ["No columns to check"],
            ),
            (
                "dq_test_unhappy_col_num",
                False,
                [
                    "Number of columns incorrect: expected 5, received 4",
                    "No datatypes checked",
                ],
            ),
            (
                "dq_test_unhappy_num_cols",
                False,
                [
                    "Number of columns incorrect: expected 5, received 4",
                    "Mismatch in datatypes, differences: "
                    "expected [('Extra', 'int')], received []",
                ],
            ),
            (
                "dq_test_unhappy_type_cols",
                False,
                [
                    "Number of columns matches: 4 expected and received",
                    "Mismatch in datatypes, differences: "
                    "expected [('Type', 'int')], received [('Type', 'string')]",
                ],
            ),
        ],
        indirect=["main_data"],
    )
    def test_columns(
        spark_session,
        main_data,
        reference_data,
        validation_output,
        expected_logging,
        caplog,
    ):
        """Test number of columns and datatypes for a given table"""
        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            dq_check_folder="test/data",
            schema_name="dq",
            run_month="",
            local=True,
        )

>       assert dq_validation.columns_datatypes().get("result", None) == validation_output
E       AssertionError: assert None == False
E        +  where None = <built-in method get of dict object at 0x000001AC9A073F00>('result', None)
E        +    where <built-in method get of dict object at 0x000001AC9A073F00> = {}.get
E        +      where {} = columns_datatypes()
E        +        where columns_datatypes = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99F64EB0>.columns_datatypes

test\test_dq\test_dq_validation.py:202: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:10 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_num_cols at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_num_cols.yml
2025-09-10 16:00:10 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_num_cols at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_num_cols.yml
2025-09-10 16:00:10 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:10 [INFO] columns_datatypes:  No columns to check
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_unhappy_num_cols at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_num_cols.yml
INFO     betl_src_poc_logger:dq_validation.py:210 No columns to check
_______________________________________________________________ test_columns[dq_test_unhappy_type_cols-False-expected_logging5] ________________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, main_data = ('dq_test_unhappy_type_cols', None), reference_data = None, validation_output = False
expected_logging = ['Number of columns matches: 4 expected and received', "Mismatch in datatypes, differences: expected [('Type', 'int')], received [('Type', 'string')]"]
caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99EB1360>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            (
                "dq_test_happy",
                True,
                ["Number of columns matches: 4 expected and received", "Datatypes match"],
            ),
            (
                "dq_test_happy_col_num",
                True,
                [
                    "Number of columns matches: 4 expected and received",
                    "No datatypes checked",
                ],
            ),
            (
                "dq_test_no_checks",
                None,
                ["No columns to check"],
            ),
            (
                "dq_test_unhappy_col_num",
                False,
                [
                    "Number of columns incorrect: expected 5, received 4",
                    "No datatypes checked",
                ],
            ),
            (
                "dq_test_unhappy_num_cols",
                False,
                [
                    "Number of columns incorrect: expected 5, received 4",
                    "Mismatch in datatypes, differences: "
                    "expected [('Extra', 'int')], received []",
                ],
            ),
            (
                "dq_test_unhappy_type_cols",
                False,
                [
                    "Number of columns matches: 4 expected and received",
                    "Mismatch in datatypes, differences: "
                    "expected [('Type', 'int')], received [('Type', 'string')]",
                ],
            ),
        ],
        indirect=["main_data"],
    )
    def test_columns(
        spark_session,
        main_data,
        reference_data,
        validation_output,
        expected_logging,
        caplog,
    ):
        """Test number of columns and datatypes for a given table"""
        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            dq_check_folder="test/data",
            schema_name="dq",
            run_month="",
            local=True,
        )

>       assert dq_validation.columns_datatypes().get("result", None) == validation_output
E       AssertionError: assert None == False
E        +  where None = <built-in method get of dict object at 0x000001AC99D8EA80>('result', None)
E        +    where <built-in method get of dict object at 0x000001AC99D8EA80> = {}.get
E        +      where {} = columns_datatypes()
E        +        where columns_datatypes = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99EB1AB0>.columns_datatypes

test\test_dq\test_dq_validation.py:202: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:10 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_type_cols at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_type_cols.yml
2025-09-10 16:00:10 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_type_cols at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_type_cols.yml
2025-09-10 16:00:10 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:10 [INFO] columns_datatypes:  No columns to check
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_unhappy_type_cols at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_type_cols.yml
INFO     betl_src_poc_logger:dq_validation.py:210 No columns to check
____________________________________________________________________ test_primary_key[dq_test_happy-True-expected_logging0] ____________________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, main_data = ('dq_test_happy', None), reference_data = None, validation_output = True
expected_logging = ["['PK1', 'PK2'] is unique", "No nulls found in ['PK1', 'PK2']", "Primary key ['PK1', 'PK2'] validated successfully"]
caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC9A0470A0>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            (
                "dq_test_happy",
                True,
                [
                    "['PK1', 'PK2'] is unique",
                    "No nulls found in ['PK1', 'PK2']",
                    "Primary key ['PK1', 'PK2'] validated successfully",
                ],
            ),
            (
                "dq_test_no_check",
                None,
                ["No Primary Key to check"],
            ),
            (
                "dq_test_unhappy_pk_dup",
                False,
                [
                    "Duplicates found in ['PK1']: [Row(PK1=2, count=2)]",
                    "No nulls found in ['PK1']",
                    "Issues found in primary key ['PK1']",
                ],
            ),
            (
                "dq_test_unhappy_pk_null",
                False,
                [
                    "['PK1', 'PK2', 'PK3'] is unique",
                    "Nulls found in not nullable column(s): ['PK3']",
                    "Issues found in primary key ['PK1', 'PK2', 'PK3']",
                ],
            ),
        ],
        indirect=["main_data"],
    )
    def test_primary_key(
        spark_session,
        main_data,
        reference_data,
        validation_output,
        expected_logging,
        caplog,
    ):
        """Test primary key nulls and uniqueness for a given table"""

        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            run_month="",
            dq_check_folder="test/data",
            local=True,
        )

>       assert dq_validation.primary_key().get("result", None) == validation_output
E       AssertionError: assert None == True
E        +  where None = <built-in method get of dict object at 0x000001AC9A0A5D80>('result', None)
E        +    where <built-in method get of dict object at 0x000001AC9A0A5D80> = {}.get
E        +      where {} = primary_key()
E        +        where primary_key = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC9A045FC0>.primary_key

test\test_dq\test_dq_validation.py:265: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:10 [INFO] __init__:  No specific checks file available for dq.dq_test_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy.yml
2025-09-10 16:00:10 [INFO] __init__:  No specific checks file available for dq.dq_test_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy.yml
2025-09-10 16:00:10 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:10 [INFO] primary_key:  No Primary Key to check
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy.yml       
INFO     betl_src_poc_logger:dq_validation.py:278 No Primary Key to check
_______________________________________________________________ test_primary_key[dq_test_unhappy_pk_dup-False-expected_logging2] _______________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, main_data = ('dq_test_unhappy_pk_dup', None), reference_data = None, validation_output = False
expected_logging = ["Duplicates found in ['PK1']: [Row(PK1=2, count=2)]", "No nulls found in ['PK1']", "Issues found in primary key ['PK1']"]
caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC9A045300>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            (
                "dq_test_happy",
                True,
                [
                    "['PK1', 'PK2'] is unique",
                    "No nulls found in ['PK1', 'PK2']",
                    "Primary key ['PK1', 'PK2'] validated successfully",
                ],
            ),
            (
                "dq_test_no_check",
                None,
                ["No Primary Key to check"],
            ),
            (
                "dq_test_unhappy_pk_dup",
                False,
                [
                    "Duplicates found in ['PK1']: [Row(PK1=2, count=2)]",
                    "No nulls found in ['PK1']",
                    "Issues found in primary key ['PK1']",
                ],
            ),
            (
                "dq_test_unhappy_pk_null",
                False,
                [
                    "['PK1', 'PK2', 'PK3'] is unique",
                    "Nulls found in not nullable column(s): ['PK3']",
                    "Issues found in primary key ['PK1', 'PK2', 'PK3']",
                ],
            ),
        ],
        indirect=["main_data"],
    )
    def test_primary_key(
        spark_session,
        main_data,
        reference_data,
        validation_output,
        expected_logging,
        caplog,
    ):
        """Test primary key nulls and uniqueness for a given table"""

        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            run_month="",
            dq_check_folder="test/data",
            local=True,
        )

>       assert dq_validation.primary_key().get("result", None) == validation_output
E       AssertionError: assert None == False
E        +  where None = <built-in method get of dict object at 0x000001AC99E50600>('result', None)
E        +    where <built-in method get of dict object at 0x000001AC99E50600> = {}.get
E        +      where {} = primary_key()
E        +        where primary_key = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC9A047E50>.primary_key

test\test_dq\test_dq_validation.py:265: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:11 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_pk_dup at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_pk_dup.yml
2025-09-10 16:00:11 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_pk_dup at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_pk_dup.yml
2025-09-10 16:00:11 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:11 [INFO] primary_key:  No Primary Key to check
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_unhappy_pk_dup at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_pk_dup.yml
INFO     betl_src_poc_logger:dq_validation.py:278 No Primary Key to check
______________________________________________________________ test_primary_key[dq_test_unhappy_pk_null-False-expected_logging3] _______________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, main_data = ('dq_test_unhappy_pk_null', None), reference_data = None, validation_output = False
expected_logging = ["['PK1', 'PK2', 'PK3'] is unique", "Nulls found in not nullable column(s): ['PK3']", "Issues found in primary key ['PK1', 'PK2', 'PK3']"]
caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99FF92D0>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            (
                "dq_test_happy",
                True,
                [
                    "['PK1', 'PK2'] is unique",
                    "No nulls found in ['PK1', 'PK2']",
                    "Primary key ['PK1', 'PK2'] validated successfully",
                ],
            ),
            (
                "dq_test_no_check",
                None,
                ["No Primary Key to check"],
            ),
            (
                "dq_test_unhappy_pk_dup",
                False,
                [
                    "Duplicates found in ['PK1']: [Row(PK1=2, count=2)]",
                    "No nulls found in ['PK1']",
                    "Issues found in primary key ['PK1']",
                ],
            ),
            (
                "dq_test_unhappy_pk_null",
                False,
                [
                    "['PK1', 'PK2', 'PK3'] is unique",
                    "Nulls found in not nullable column(s): ['PK3']",
                    "Issues found in primary key ['PK1', 'PK2', 'PK3']",
                ],
            ),
        ],
        indirect=["main_data"],
    )
    def test_primary_key(
        spark_session,
        main_data,
        reference_data,
        validation_output,
        expected_logging,
        caplog,
    ):
        """Test primary key nulls and uniqueness for a given table"""

        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            run_month="",
            dq_check_folder="test/data",
            local=True,
        )

>       assert dq_validation.primary_key().get("result", None) == validation_output
E       AssertionError: assert None == False
E        +  where None = <built-in method get of dict object at 0x000001AC99E3C340>('result', None)
E        +    where <built-in method get of dict object at 0x000001AC99E3C340> = {}.get
E        +      where {} = primary_key()
E        +        where primary_key = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99FFA410>.primary_key

test\test_dq\test_dq_validation.py:265: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:11 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_pk_null at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_pk_null.yml
2025-09-10 16:00:11 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_pk_null at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_pk_null.yml
2025-09-10 16:00:11 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:11 [INFO] primary_key:  No Primary Key to check
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_unhappy_pk_null at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_pk_null.yml
INFO     betl_src_poc_logger:dq_validation.py:278 No Primary Key to check
_____________________________________________________________________ test_not_nulls[dq_test_happy-True-expected_logging0] _____________________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, main_data = ('dq_test_happy', None), reference_data = None, validation_output = True
expected_logging = ["No nulls found in ['PK1']"], caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99CF5240>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            (
                "dq_test_happy",
                True,
                ["No nulls found in ['PK1']"],
            ),
            (
                "dq_test_no_check",
                None,
                ["No not nullable columns to check"],
            ),
            (
                "dq_test_unhappy_not_null",
                False,
                ["Nulls found in not nullable column(s): ['Type']"],
            ),
        ],
        indirect=["main_data"],
    )
    def test_not_nulls(
        spark_session,
        main_data,
        reference_data,
        validation_output,
        expected_logging,
        caplog,
    ):
        """Test not nullable columns for a given table"""

        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            run_month="",
            dq_check_folder="test/data",
            local=True,
        )

>       assert dq_validation.not_null().get("result", None) == validation_output
E       AssertionError: assert None == True
E        +  where None = <built-in method get of dict object at 0x000001AC99DA8700>('result', None)
E        +    where <built-in method get of dict object at 0x000001AC99DA8700> = {}.get
E        +      where {} = not_null()
E        +        where not_null = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99CF4820>.not_null

test\test_dq\test_dq_validation.py:311: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:11 [INFO] __init__:  No specific checks file available for dq.dq_test_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy.yml
2025-09-10 16:00:11 [INFO] __init__:  No specific checks file available for dq.dq_test_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy.yml
2025-09-10 16:00:11 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:11 [INFO] not_null:  No not nullable columns to check
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy.yml       
INFO     betl_src_poc_logger:dq_validation.py:387 No not nullable columns to check
_______________________________________________________________ test_not_nulls[dq_test_unhappy_not_null-False-expected_logging2] _______________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, main_data = ('dq_test_unhappy_not_null', None), reference_data = None, validation_output = False
expected_logging = ["Nulls found in not nullable column(s): ['Type']"], caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99F96620>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            (
                "dq_test_happy",
                True,
                ["No nulls found in ['PK1']"],
            ),
            (
                "dq_test_no_check",
                None,
                ["No not nullable columns to check"],
            ),
            (
                "dq_test_unhappy_not_null",
                False,
                ["Nulls found in not nullable column(s): ['Type']"],
            ),
        ],
        indirect=["main_data"],
    )
    def test_not_nulls(
        spark_session,
        main_data,
        reference_data,
        validation_output,
        expected_logging,
        caplog,
    ):
        """Test not nullable columns for a given table"""

        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            run_month="",
            dq_check_folder="test/data",
            local=True,
        )

>       assert dq_validation.not_null().get("result", None) == validation_output
E       AssertionError: assert None == False
E        +  where None = <built-in method get of dict object at 0x000001AC99E8F000>('result', None)
E        +    where <built-in method get of dict object at 0x000001AC99E8F000> = {}.get
E        +      where {} = not_null()
E        +        where not_null = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99F954E0>.not_null

test\test_dq\test_dq_validation.py:311: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:11 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_not_null at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_not_null.yml
2025-09-10 16:00:11 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_not_null at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_not_null.yml
2025-09-10 16:00:11 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:11 [INFO] not_null:  No not nullable columns to check
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_unhappy_not_null at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_not_null.yml
INFO     betl_src_poc_logger:dq_validation.py:387 No not nullable columns to check
_______________________________________________________________ test_referential_integrity[dq_test_happy-True-expected_logging0] _______________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, main_data = ('dq_test_happy', None), reference_data = None, validation_output = True
expected_logging = ["Referential check successful, all values from Type with filter 'Type is not null' are present in reference.Lookup1"]
caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99D03040>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            (
                "dq_test_happy",
                True,
                [
                    "Referential check successful, all values from Type "
                    "with filter 'Type is not null' are present in reference.Lookup1"
                ],
            ),
            (
                "dq_test_no_check",
                None,
                ["No referential integrity checks"],
            ),
            (
                "dq_test_unhappy_ref",
                False,
                [
                    "Referential check failed, not all values from Type "
                    "with filter 'Type is not null' are present in reference.Lookup2\n"
                    "1 values not available: ['B']"
                ],
            ),
            (
                "dq_test_unhappy_ref_filter",
                False,
                [
                    "Referential check failed, not all values from Type are present in reference.Lookup1\n1 values not available: [None]"  # noqa: E501
                ],
            ),
        ],
        indirect=["main_data"],
    )
    def test_referential_integrity(
        spark_session,
        main_data,
        reference_data,
        validation_output,
        expected_logging,
        caplog,
    ):
        """Test referential integrity for a given table"""

        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            run_month="",
            dq_check_folder="test/data",
            local=True,
        )

>       assert (
            dq_validation.referential_integrity().get("result", None) == validation_output
        )
E       AssertionError: assert None == True
E        +  where None = <built-in method get of dict object at 0x000001AC99F4C980>('result', None)
E        +    where <built-in method get of dict object at 0x000001AC99F4C980> = {}.get
E        +      where {} = referential_integrity()
E        +        where referential_integrity = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99D00940>.referential_integrity

test\test_dq\test_dq_validation.py:371: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:11 [INFO] __init__:  No specific checks file available for dq.dq_test_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy.yml
2025-09-10 16:00:11 [INFO] __init__:  No specific checks file available for dq.dq_test_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy.yml
2025-09-10 16:00:11 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:11 [INFO] referential_integrity:  No referential integrity checks
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy.yml       
INFO     betl_src_poc_logger:dq_validation.py:427 No referential integrity checks
___________________________________________________________ test_referential_integrity[dq_test_unhappy_ref-False-expected_logging2] ____________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, main_data = ('dq_test_unhappy_ref', None), reference_data = None, validation_output = False
expected_logging = ["Referential check failed, not all values from Type with filter 'Type is not null' are present in reference.Lookup2\n1 values not available: ['B']"]
caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99F957E0>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            (
                "dq_test_happy",
                True,
                [
                    "Referential check successful, all values from Type "
                    "with filter 'Type is not null' are present in reference.Lookup1"
                ],
            ),
            (
                "dq_test_no_check",
                None,
                ["No referential integrity checks"],
            ),
            (
                "dq_test_unhappy_ref",
                False,
                [
                    "Referential check failed, not all values from Type "
                    "with filter 'Type is not null' are present in reference.Lookup2\n"
                    "1 values not available: ['B']"
                ],
            ),
            (
                "dq_test_unhappy_ref_filter",
                False,
                [
                    "Referential check failed, not all values from Type are present in reference.Lookup1\n1 values not available: [None]"  # noqa: E501
                ],
            ),
        ],
        indirect=["main_data"],
    )
    def test_referential_integrity(
        spark_session,
        main_data,
        reference_data,
        validation_output,
        expected_logging,
        caplog,
    ):
        """Test referential integrity for a given table"""

        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            run_month="",
            dq_check_folder="test/data",
            local=True,
        )

>       assert (
            dq_validation.referential_integrity().get("result", None) == validation_output
        )
E       AssertionError: assert None == False
E        +  where None = <built-in method get of dict object at 0x000001AC9A089C80>('result', None)
E        +    where <built-in method get of dict object at 0x000001AC9A089C80> = {}.get
E        +      where {} = referential_integrity()
E        +        where referential_integrity = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99F96410>.referential_integrity

test\test_dq\test_dq_validation.py:371: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:11 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_ref at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_ref.yml      
2025-09-10 16:00:11 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_ref at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_ref.yml      
2025-09-10 16:00:11 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:11 [INFO] referential_integrity:  No referential integrity checks
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_unhappy_ref at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_ref.yml
INFO     betl_src_poc_logger:dq_validation.py:427 No referential integrity checks
________________________________________________________ test_referential_integrity[dq_test_unhappy_ref_filter-False-expected_logging3] ________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, main_data = ('dq_test_unhappy_ref_filter', None), reference_data = None, validation_output = False
expected_logging = ['Referential check failed, not all values from Type are present in reference.Lookup1\n1 values not available: [None]']
caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99E740A0>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            (
                "dq_test_happy",
                True,
                [
                    "Referential check successful, all values from Type "
                    "with filter 'Type is not null' are present in reference.Lookup1"
                ],
            ),
            (
                "dq_test_no_check",
                None,
                ["No referential integrity checks"],
            ),
            (
                "dq_test_unhappy_ref",
                False,
                [
                    "Referential check failed, not all values from Type "
                    "with filter 'Type is not null' are present in reference.Lookup2\n"
                    "1 values not available: ['B']"
                ],
            ),
            (
                "dq_test_unhappy_ref_filter",
                False,
                [
                    "Referential check failed, not all values from Type are present in reference.Lookup1\n1 values not available: [None]"  # noqa: E501
                ],
            ),
        ],
        indirect=["main_data"],
    )
    def test_referential_integrity(
        spark_session,
        main_data,
        reference_data,
        validation_output,
        expected_logging,
        caplog,
    ):
        """Test referential integrity for a given table"""

        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            run_month="",
            dq_check_folder="test/data",
            local=True,
        )

>       assert (
            dq_validation.referential_integrity().get("result", None) == validation_output
        )
E       AssertionError: assert None == False
E        +  where None = <built-in method get of dict object at 0x000001AC99D47680>('result', None)
E        +    where <built-in method get of dict object at 0x000001AC99D47680> = {}.get
E        +      where {} = referential_integrity()
E        +        where referential_integrity = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99E76AD0>.referential_integrity

test\test_dq\test_dq_validation.py:371: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:12 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_ref_filter at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_ref_filter.yml
2025-09-10 16:00:12 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_ref_filter at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_ref_filter.yml
2025-09-10 16:00:12 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:12 [INFO] referential_integrity:  No referential integrity checks
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_unhappy_ref_filter at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_ref_filter.yml
INFO     betl_src_poc_logger:dq_validation.py:427 No referential integrity checks
______________________________________________________________________ test_unique[dq_test_happy-True-expected_logging0] _______________________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, main_data = ('dq_test_happy', None), validation_output = True, expected_logging = ["['PK2'] is unique"]
caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99F94E80>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            (
                "dq_test_happy",
                True,
                ["['PK2'] is unique"],
            ),
            (
                "dq_test_no_check",
                None,
                ["No unique columns to check"],
            ),
            (
                "dq_test_unhappy_unique",
                False,
                ["Duplicates found in ['PK1']: [Row(PK1=2, count=2)]", "['PK2'] is unique"],
            ),
        ],
        indirect=["main_data"],
    )
    def test_unique(
        spark_session,
        main_data,
        validation_output,
        expected_logging,
        caplog,
    ):
        """Test full DQ validation for a given table"""

        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            dq_check_folder="test/data",
            run_month="",
            local=True,
        )

>       assert (
            dq_validation.unique(individual=True).get("result", None) == validation_output
        )
E       AssertionError: assert None == True
E        +  where None = <built-in method get of dict object at 0x000001AC99D8CB80>('result', None)
E        +    where <built-in method get of dict object at 0x000001AC99D8CB80> = {}.get
E        +      where {} = unique(individual=True)
E        +        where unique = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99F94BB0>.unique

test\test_dq\test_dq_validation.py:418: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:12 [INFO] __init__:  No specific checks file available for dq.dq_test_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy.yml
2025-09-10 16:00:12 [INFO] __init__:  No specific checks file available for dq.dq_test_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy.yml
2025-09-10 16:00:12 [INFO] unique:  No unique columns to check
2025-09-10 16:00:12 [INFO] unique:  No unique columns to check
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_happy.yml       
INFO     betl_src_poc_logger:dq_validation.py:333 No unique columns to check
_________________________________________________________________ test_unique[dq_test_unhappy_unique-False-expected_logging2] __________________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, main_data = ('dq_test_unhappy_unique', None), validation_output = False
expected_logging = ["Duplicates found in ['PK1']: [Row(PK1=2, count=2)]", "['PK2'] is unique"], caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99EBD900>

    @pytest.mark.parametrize(
        ("main_data", "validation_output", "expected_logging"),
        [
            (
                "dq_test_happy",
                True,
                ["['PK2'] is unique"],
            ),
            (
                "dq_test_no_check",
                None,
                ["No unique columns to check"],
            ),
            (
                "dq_test_unhappy_unique",
                False,
                ["Duplicates found in ['PK1']: [Row(PK1=2, count=2)]", "['PK2'] is unique"],
            ),
        ],
        indirect=["main_data"],
    )
    def test_unique(
        spark_session,
        main_data,
        validation_output,
        expected_logging,
        caplog,
    ):
        """Test full DQ validation for a given table"""

        table_name = main_data[0]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            dq_check_folder="test/data",
            run_month="",
            local=True,
        )

>       assert (
            dq_validation.unique(individual=True).get("result", None) == validation_output
        )
E       AssertionError: assert None == False
E        +  where None = <built-in method get of dict object at 0x000001AC99F86E00>('result', None)
E        +    where <built-in method get of dict object at 0x000001AC99F86E00> = {}.get
E        +      where {} = unique(individual=True)
E        +        where unique = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99EBC250>.unique

test\test_dq\test_dq_validation.py:418: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:12 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_unique at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_unique.yml
2025-09-10 16:00:12 [INFO] __init__:  No specific checks file available for dq.dq_test_unhappy_unique at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_unique.yml
2025-09-10 16:00:12 [INFO] unique:  No unique columns to check
2025-09-10 16:00:12 [INFO] unique:  No unique columns to check
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_unhappy_unique at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_unhappy_unique.yml
INFO     betl_src_poc_logger:dq_validation.py:333 No unique columns to check
_____________________________________________________________________ test_generic_checks[dq_test_fr_happy-fr-log_range0] ______________________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, main_data = ('dq_test_fr_happy', None), reference_data = None, source_system = 'fr'
log_range = [2, 3, 4, 5, 6, 7, ...], caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99FFB130>

    @pytest.mark.parametrize(
        ("main_data", "source_system", "log_range"),
        [
            ("dq_test_fr_happy", "fr", [*list(range(2, 10)), 11]),
            ("dq_test_fr_no_generic", "fr", [*list(range(1, 9)), 10, 11]),
            ("dq_test_nospecific_happy", "nospecific", [0, *list(range(2, 9)), 10, 11]),
        ],
        indirect=["main_data"],
    )
    def test_generic_checks(
        spark_session, main_data, reference_data, source_system, log_range, caplog
    ):
        table_name = main_data[0]

        log = [
            "No specific checks file available for "
            "dq.dq_test_nospecific_happy at "
            + str(
                Path(
                    MAPPING_ROOT_DIR
                    / "test"
                    / "data"
                    / "dq"
                    / "dq_test_nospecific_happy.yml"
                )
            ),
            "No generic checks file available for "
            "dq.dq_test_fr_no_generic at "
            + str(
                Path(
                    MAPPING_ROOT_DIR / "test" / "data" / "dq" / "dq_test_[]_no_generic.yml"
                )
            ),
            "Number of columns matches: 4 expected and received",
            "Datatypes match",
            "['PK1', 'PK2'] is unique",
            "No nulls found in ['PK1', 'PK2']",
            "Primary key ['PK1', 'PK2'] validated successfully",
            "No nulls found in ['PK1']",
            "['PK2'] is unique",
            "Referential check successful, "
            "all values from Type with filter 'Type is not null' "
            "are present in reference.Lookup1",
            "No referential integrity checks",
            "Checks completed successfully",
        ]

        log = [log[i] for i in log_range]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            dq_check_folder="test/data",
            run_month="",
            source_system=source_system,
            local=True,
        )

>       assert dq_validation.checks(functional=True)
E       assert None
E        +  where None = checks(functional=True)
E        +    where checks = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99FF9E40>.checks

test\test_dq\test_dq_validation.py:483: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:12 [INFO] __init__:  No generic checks file available for dq.dq_test_fr_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_[]_happy.yml
2025-09-10 16:00:12 [INFO] __init__:  No generic checks file available for dq.dq_test_fr_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_[]_happy.yml
2025-09-10 16:00:12 [INFO] __init__:  No specific checks file available for dq.dq_test_fr_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_fr_happy.yml
2025-09-10 16:00:12 [INFO] __init__:  No specific checks file available for dq.dq_test_fr_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_fr_happy.yml
2025-09-10 16:00:12 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:12 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:12 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:12 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:12 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:12 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:12 [INFO] unique:  No unique columns to check
2025-09-10 16:00:12 [INFO] unique:  No unique columns to check
2025-09-10 16:00:12 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:12 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:12 [INFO] checks:  No checks done
2025-09-10 16:00:12 [INFO] checks:  No checks done
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:71 No generic checks file available for dq.dq_test_fr_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_[]_happy.yml  
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_fr_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_fr_happy.yml 
INFO     betl_src_poc_logger:dq_validation.py:210 No columns to check
INFO     betl_src_poc_logger:dq_validation.py:278 No Primary Key to check
INFO     betl_src_poc_logger:dq_validation.py:387 No not nullable columns to check
INFO     betl_src_poc_logger:dq_validation.py:333 No unique columns to check
INFO     betl_src_poc_logger:dq_validation.py:427 No referential integrity checks
INFO     betl_src_poc_logger:dq_validation.py:171 No checks done
___________________________________________________________________ test_generic_checks[dq_test_fr_no_generic-fr-log_range1] ___________________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, main_data = ('dq_test_fr_no_generic', None), reference_data = None, source_system = 'fr'
log_range = [1, 2, 3, 4, 5, 6, ...], caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99EBED70>

    @pytest.mark.parametrize(
        ("main_data", "source_system", "log_range"),
        [
            ("dq_test_fr_happy", "fr", [*list(range(2, 10)), 11]),
            ("dq_test_fr_no_generic", "fr", [*list(range(1, 9)), 10, 11]),
            ("dq_test_nospecific_happy", "nospecific", [0, *list(range(2, 9)), 10, 11]),
        ],
        indirect=["main_data"],
    )
    def test_generic_checks(
        spark_session, main_data, reference_data, source_system, log_range, caplog
    ):
        table_name = main_data[0]
    
        log = [
            "No specific checks file available for "
            "dq.dq_test_nospecific_happy at "
            + str(
                Path(
                    MAPPING_ROOT_DIR
                    / "test"
                    / "data"
                    / "dq"
                    / "dq_test_nospecific_happy.yml"
                )
            ),
            "No generic checks file available for "
            "dq.dq_test_fr_no_generic at "
            + str(
                Path(
                    MAPPING_ROOT_DIR / "test" / "data" / "dq" / "dq_test_[]_no_generic.yml"
                )
            ),
            "Number of columns matches: 4 expected and received",
            "Datatypes match",
            "['PK1', 'PK2'] is unique",
            "No nulls found in ['PK1', 'PK2']",
            "Primary key ['PK1', 'PK2'] validated successfully",
            "No nulls found in ['PK1']",
            "['PK2'] is unique",
            "Referential check successful, "
            "all values from Type with filter 'Type is not null' "
            "are present in reference.Lookup1",
            "No referential integrity checks",
            "Checks completed successfully",
        ]

        log = [log[i] for i in log_range]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            dq_check_folder="test/data",
            run_month="",
            source_system=source_system,
            local=True,
        )

>       assert dq_validation.checks(functional=True)
E       assert None
E        +  where None = checks(functional=True)
E        +    where checks = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99EBC7C0>.checks

test\test_dq\test_dq_validation.py:483: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:12 [INFO] __init__:  No generic checks file available for dq.dq_test_fr_no_generic at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_[]_no_generic.yml   
2025-09-10 16:00:12 [INFO] __init__:  No generic checks file available for dq.dq_test_fr_no_generic at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_[]_no_generic.yml   
2025-09-10 16:00:12 [INFO] __init__:  No specific checks file available for dq.dq_test_fr_no_generic at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_fr_no_generic.yml  
2025-09-10 16:00:12 [INFO] __init__:  No specific checks file available for dq.dq_test_fr_no_generic at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_fr_no_generic.yml  
2025-09-10 16:00:12 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:12 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:12 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:12 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:12 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:12 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:12 [INFO] unique:  No unique columns to check
2025-09-10 16:00:12 [INFO] unique:  No unique columns to check
2025-09-10 16:00:12 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:12 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:12 [INFO] checks:  No checks done
2025-09-10 16:00:12 [INFO] checks:  No checks done
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:71 No generic checks file available for dq.dq_test_fr_no_generic at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_[]_no_generic.yml
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_fr_no_generic at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_fr_no_generic.yml
INFO     betl_src_poc_logger:dq_validation.py:210 No columns to check
INFO     betl_src_poc_logger:dq_validation.py:278 No Primary Key to check
INFO     betl_src_poc_logger:dq_validation.py:387 No not nullable columns to check
INFO     betl_src_poc_logger:dq_validation.py:333 No unique columns to check
INFO     betl_src_poc_logger:dq_validation.py:427 No referential integrity checks
INFO     betl_src_poc_logger:dq_validation.py:171 No checks done
_____________________________________________________________ test_generic_checks[dq_test_nospecific_happy-nospecific-log_range2] ______________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>, main_data = ('dq_test_nospecific_happy', None), reference_data = None, source_system = 'nospecific'
log_range = [0, 2, 3, 4, 5, 6, ...], caplog = <_pytest.logging.LogCaptureFixture object at 0x000001AC99FF84C0>

    @pytest.mark.parametrize(
        ("main_data", "source_system", "log_range"),
        [
            ("dq_test_fr_happy", "fr", [*list(range(2, 10)), 11]),
            ("dq_test_fr_no_generic", "fr", [*list(range(1, 9)), 10, 11]),
            ("dq_test_nospecific_happy", "nospecific", [0, *list(range(2, 9)), 10, 11]),
        ],
        indirect=["main_data"],
    )
    def test_generic_checks(
        spark_session, main_data, reference_data, source_system, log_range, caplog
    ):
        table_name = main_data[0]

        log = [
            "No specific checks file available for "
            "dq.dq_test_nospecific_happy at "
            + str(
                Path(
                    MAPPING_ROOT_DIR
                    / "test"
                    / "data"
                    / "dq"
                    / "dq_test_nospecific_happy.yml"
                )
            ),
            "No generic checks file available for "
            "dq.dq_test_fr_no_generic at "
            + str(
                Path(
                    MAPPING_ROOT_DIR / "test" / "data" / "dq" / "dq_test_[]_no_generic.yml"
                )
            ),
            "Number of columns matches: 4 expected and received",
            "Datatypes match",
            "['PK1', 'PK2'] is unique",
            "No nulls found in ['PK1', 'PK2']",
            "Primary key ['PK1', 'PK2'] validated successfully",
            "No nulls found in ['PK1']",
            "['PK2'] is unique",
            "Referential check successful, "
            "all values from Type with filter 'Type is not null' "
            "are present in reference.Lookup1",
            "No referential integrity checks",
            "Checks completed successfully",
        ]

        log = [log[i] for i in log_range]

        dq_validation = DQValidation(
            spark_session,
            table_name,
            schema_name="dq",
            dq_check_folder="test/data",
            run_month="",
            source_system=source_system,
            local=True,
        )

>       assert dq_validation.checks(functional=True)
E       assert None
E        +  where None = checks(functional=True)
E        +    where checks = <abnamro_bsrc_etl.dq.dq_validation.DQValidation object at 0x000001AC99FFB400>.checks

test\test_dq\test_dq_validation.py:483: AssertionError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:00:12 [INFO] __init__:  No generic checks file available for dq.dq_test_nospecific_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_[]_happy.yml     
2025-09-10 16:00:12 [INFO] __init__:  No generic checks file available for dq.dq_test_nospecific_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_[]_happy.yml     
2025-09-10 16:00:12 [INFO] __init__:  No specific checks file available for dq.dq_test_nospecific_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_nospecific_happy.yml
2025-09-10 16:00:12 [INFO] __init__:  No specific checks file available for dq.dq_test_nospecific_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_nospecific_happy.yml
2025-09-10 16:00:12 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:12 [INFO] columns_datatypes:  No columns to check
2025-09-10 16:00:12 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:12 [INFO] primary_key:  No Primary Key to check
2025-09-10 16:00:12 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:12 [INFO] not_null:  No not nullable columns to check
2025-09-10 16:00:12 [INFO] unique:  No unique columns to check
2025-09-10 16:00:12 [INFO] unique:  No unique columns to check
2025-09-10 16:00:12 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:12 [INFO] referential_integrity:  No referential integrity checks
2025-09-10 16:00:12 [INFO] checks:  No checks done
2025-09-10 16:00:12 [INFO] checks:  No checks done
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:dq_validation.py:71 No generic checks file available for dq.dq_test_nospecific_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_[]_happy.yml
INFO     betl_src_poc_logger:dq_validation.py:87 No specific checks file available for dq.dq_test_nospecific_happy at C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test_data\test\data\dq\dq_test_nospecific_happy.yml
INFO     betl_src_poc_logger:dq_validation.py:210 No columns to check
INFO     betl_src_poc_logger:dq_validation.py:278 No Primary Key to check
INFO     betl_src_poc_logger:dq_validation.py:387 No not nullable columns to check
INFO     betl_src_poc_logger:dq_validation.py:333 No unique columns to check
INFO     betl_src_poc_logger:dq_validation.py:427 No referential integrity checks
INFO     betl_src_poc_logger:dq_validation.py:171 No checks done
_________________________________________ test_pipeline_yaml_integrated_target[parameters0-test_catalog.test_schema_20240801.testd1_test_target_table] _________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>
source_data = {'table_a': {'data': [(1, 2, 3, 1, 5, 1, ...), (2, 3, 3, 1, 5, 2, ...), (3, 1, 3, 0, 5, 3, ...)], 'schema': ['col01', ...col09b']}, 'table_c_testd1': {'data': [(1, 10, 11), (2, 10, 11), (3, 10, 11)], 'schema': ['col01c', 'col11', 'col12']}}
parameters = {'DELIVERY_ENTITY': 'TEST-D1', 'RUN_MONTH': '20240801'}, target_table_name = 'test_catalog.test_schema_20240801.testd1_test_target_table'

    @pytest.mark.parametrize(
        ("parameters", "target_table_name"),
        [
            (
                {"RUN_MONTH": "20240801", "DELIVERY_ENTITY": "TEST-D1"},
                "test_catalog.test_schema_20240801.testd1_test_target_table",
            ),
        ],
    )
    def test_pipeline_yaml_integrated_target(
        spark_session, source_data, parameters, target_table_name
    ):
        """Test full pipeline of YAML, Integrated data and (filtered) target attributes.

        Check `test/data/TEST_YAML.yml` for:
        - `description`
        - `sources`
        - `transformations`
        - `expressions`
        - `filter_target`
        """
>       business_logic = parse_yaml("../test/data/TEST_YAML.yml", parameters=parameters)

test\test_transform\test_pipeline_yaml_integrated_target.py:80:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  
src\abnamro_bsrc_etl\utils\parse_yaml.py:32: in parse_yaml
    with Path.open(MAPPING_ROOT_DIR / "business_logic" / yaml_path) as yaml_file:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  

self = WindowsPath('C:/Users/B25712/bsrc-etl-venv/bsrc-etl/test_data/business_logic/../test/data/TEST_YAML.yml'), mode = 'r', buffering = -1, encoding = 'locale', errors = None, newline = None 

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return self._accessor.open(self, mode, buffering, encoding, errors,
                                   newline)
E       FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\B25712\\bsrc-etl-venv\\bsrc-etl\\test_data\\business_logic\\..\\test\\data\\TEST_YAML.yml'

C:\Program Files\Python310\lib\pathlib.py:1119: FileNotFoundError
___________________________________________________________________________________ test_pipeline_yaml_pivot ___________________________________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>

    def test_pipeline_yaml_pivot(spark_session):
>       business_logic = parse_yaml("../test/data/TEST_YAML_PIVOT.yml")

test\test_transform\test_pipeline_yaml_integrated_target.py:210:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  
src\abnamro_bsrc_etl\utils\parse_yaml.py:32: in parse_yaml
    with Path.open(MAPPING_ROOT_DIR / "business_logic" / yaml_path) as yaml_file:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  

self = WindowsPath('C:/Users/B25712/bsrc-etl-venv/bsrc-etl/test_data/business_logic/../test/data/TEST_YAML_PIVOT.yml'), mode = 'r', buffering = -1, encoding = 'locale', errors = None
newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return self._accessor.open(self, mode, buffering, encoding, errors,
                                   newline)
E       FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\B25712\\bsrc-etl-venv\\bsrc-etl\\test_data\\business_logic\\..\\test\\data\\TEST_YAML_PIVOT.yml'

C:\Program Files\Python310\lib\pathlib.py:1119: FileNotFoundError
_______________________________________________________________________________________ test_run_mapping _______________________________________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001AC99A473A0>

    def test_run_mapping(spark_session):
        """Test full pipeline from run_mapping script"""

        source_dict = {
            "table_a": {
                "data": [("a", "b", 2), ("a", "b", 4)],
                "schema": ["col01", "col02", "col03"],
            },
            "table_b": {
                "data": [("a", "X"), ("a", "Y"), ("b", "Z")],
                "schema": ["col01", "col04"],
            },
            "table_c": {
                "data": [(2, 1.3, 1.8), (2, 2.6, 3.5)],
                "schema": ["X", "col05", "col06"],
            },
        }
        for source_name, source_data in source_dict.items():
            spark_session.createDataFrame(**source_data).createOrReplaceTempView(
                source_name
            )

>       run_mapping(
            spark_session,
            stage="test/data",
            target_mapping="TEST_YAML_PIVOT.yml",
            run_month="",
            local=True,
        )

test\test_transform\test_pipeline_yaml_integrated_target.py:288:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  
src\abnamro_bsrc_etl\scripts\run_mapping.py:60: in run_mapping
    business_logic_dict = parse_yaml(
src\abnamro_bsrc_etl\utils\parse_yaml.py:32: in parse_yaml
    with Path.open(MAPPING_ROOT_DIR / "business_logic" / yaml_path) as yaml_file:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  

self = WindowsPath('C:/Users/B25712/bsrc-etl-venv/bsrc-etl/test_data/test/data/TEST_YAML_PIVOT.yml'), mode = 'r', buffering = -1, encoding = 'locale', errors = None, newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return self._accessor.open(self, mode, buffering, encoding, errors,
                                   newline)
E       FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\B25712\\bsrc-etl-venv\\bsrc-etl\\test_data\\test\\data\\TEST_YAML_PIVOT.yml'

C:\Program Files\Python310\lib\pathlib.py:1119: FileNotFoundError
------------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------- 
2025-09-10 16:03:20 [ERROR] write_to_log:  Error writing to process log table log_.process_log
Traceback (most recent call last):
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl\src\abnamro_bsrc_etl\utils\table_logging.py", line 33, in write_to_log
    log_entry_df.write.mode("append").saveAsTable(
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl-venv\lib\site-packages\pyspark\sql\readwriter.py", line 1586, in saveAsTable
    self._jwrite.saveAsTable(name)
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl-venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl-venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Couldn't find a catalog to handle the identifier bsrc_d.log_.process_log.
2025-09-10 16:03:20 [ERROR] write_to_log:  Error writing to process log table log_.process_log
Traceback (most recent call last):
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl\src\abnamro_bsrc_etl\utils\table_logging.py", line 33, in write_to_log
    log_entry_df.write.mode("append").saveAsTable(
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl-venv\lib\site-packages\pyspark\sql\readwriter.py", line 1586, in saveAsTable
    self._jwrite.saveAsTable(name)
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl-venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl-venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Couldn't find a catalog to handle the identifier bsrc_d.log_.process_log.
-------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------- 
ERROR    betl_src_poc_logger:table_logging.py:37 Error writing to process log table log_.process_log
Traceback (most recent call last):
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl\src\abnamro_bsrc_etl\utils\table_logging.py", line 33, in write_to_log
    log_entry_df.write.mode("append").saveAsTable(
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl-venv\lib\site-packages\pyspark\sql\readwriter.py", line 1586, in saveAsTable
    self._jwrite.saveAsTable(name)
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl-venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl-venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Couldn't find a catalog to handle the identifier bsrc_d.log_.process_log.
___________________________________________________________________________ test_parse_yaml[IHUB-FR1-202412-output0] ___________________________________________________________________________ 

delivery_entity = 'IHUB-FR1', run_month = '202412'
output = {'expressions': {'DeliveryEntity': "'IHUB-FR1'", 'DeliveryEntity2': "'IHUB-FR1'", 'NewCol01': 'TBLA.col01', 'NewCol11'...BLA', 'right_source': 'TBLC'}}, {'add_variables': {'column_mapping': {'var': "colA in ('''ABC''', '''IHUB-FR1''')"}}}]}

    @pytest.mark.parametrize(
        ("delivery_entity", "run_month", "output"),
        [
            (
                "IHUB-FR1",
                "202412",
                {
                    "target": "test_catalog.test_schema_202412.ihubfr1_test_target_table",
                    "sources": [
                        {
                            "alias": "TBLA",
                            "columns": ["col01", "col02", "col03"],
                            "filter": "col02 = '''IHUB-FR1''' AND col03 = '''IHUB-FR1'''",
                            "source": "schema1.ihubfr1_table_a",
                        },
                        {
                            "alias": "TBLC",
                            "columns": ["col01c", "col11", "col12"],
                            "source": "schema2.ihubfr1_table_c",
                        },
                    ],
                    "transformations": [
                        {
                            "join": {
                                "left_source": "TBLA",
                                "right_source": "TBLC",
                                "condition": [
                                    "TBLA.col01 = TBLC.col01c",
                                    "TBLC.col12 = '''IHUB-FR1'''",
                                    "TBLC.col12 = '''IHUB-FR1'''",
                                ],
                                "how": "left",
                            }
                        },
                        {
                            "add_variables": {
                                "column_mapping": {
                                    "var": "colA in ('''ABC''', '''IHUB-FR1''')"
                                }
                            }
                        },
                    ],
                    "expressions": {
                        "DeliveryEntity": "'IHUB-FR1'",
                        "DeliveryEntity2": "'IHUB-FR1'",
                        "WrongDeliveryEntity": "ihubfr1",
                        "NewCol01": "TBLA.col01",
                        "NewCol11": "TBLC.col11",
                    },
                },
            ),
        ],
    )
    def test_parse_yaml(delivery_entity, run_month, output):
        """Test Parse YAML file."""
>       business_logic = parse_yaml(
            "../test/data/test_parse_yaml.yml",
            parameters={"DELIVERY_ENTITY": delivery_entity, "RUN_MONTH": run_month},
        )

test\test_utils\test_parse_yaml.py:61:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  
src\abnamro_bsrc_etl\utils\parse_yaml.py:32: in parse_yaml
    with Path.open(MAPPING_ROOT_DIR / "business_logic" / yaml_path) as yaml_file:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  

self = WindowsPath('C:/Users/B25712/bsrc-etl-venv/bsrc-etl/test_data/business_logic/../test/data/test_parse_yaml.yml'), mode = 'r', buffering = -1, encoding = 'locale', errors = None
newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return self._accessor.open(self, mode, buffering, encoding, errors,
                                   newline)
E       FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\B25712\\bsrc-etl-venv\\bsrc-etl\\test_data\\business_logic\\..\\test\\data\\test_parse_yaml.yml'

C:\Program Files\Python310\lib\pathlib.py:1119: FileNotFoundError
_______________________________________________________________________________________ test_get_schema ________________________________________________________________________________________ 

    def test_get_schema():
        schema = StructType(
            [
                StructField("MainIdentifier", LongType(), nullable=True),
                StructField("MaxCol3", LongType(), nullable=True),
                StructField("MedCol12", DoubleType(), nullable=True),
                StructField("AvgCol09", DoubleType(), nullable=True),
                StructField("NullInCase", StringType(), nullable=False),
                StructField("StrInCase", StringType(), nullable=False),
                StructField("ConstEmptyString", StringType(), nullable=False),
                StructField("ConstInt", IntegerType(), nullable=False),
                StructField("ConstString", StringType(), nullable=False),
            ]
        )
>       assert (
            get_schema_from_file(uc_schema="../test/data", table_name="TEST_SCHEMA")
            == schema
        )

test\test_utils\test_table_schema.py:27:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src\abnamro_bsrc_etl\utils\table_schema.py:13: in get_schema_from_file
    with Path.open(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  

self = WindowsPath('C:/Users/B25712/bsrc-etl-venv/bsrc-etl/test_data/table_structures/../test/data/TEST_SCHEMA.json'), mode = 'r', buffering = -1, encoding = 'locale', errors = None
newline = None

    def open(self, mode='r', buffering=-1, encoding=None,
             errors=None, newline=None):
        """
        Open the file pointed by this path and return a file object, as
        the built-in open() function does.
        """
        if "b" not in mode:
            encoding = io.text_encoding(encoding)
>       return self._accessor.open(self, mode, buffering, encoding, errors,
                                   newline)
E       FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\B25712\\bsrc-etl-venv\\bsrc-etl\\test_data\\table_structures\\..\\test\\data\\TEST_SCHEMA.json'

C:\Program Files\Python310\lib\pathlib.py:1119: FileNotFoundError
======================================================================================= warnings summary ======================================================================================= 
..\bsrc-etl-venv\lib\site-packages\holidays\deprecations\v1_incompatibility.py:40
  C:\Users\B25712\bsrc-etl-venv\bsrc-etl-venv\lib\site-packages\holidays\deprecations\v1_incompatibility.py:40: FutureIncompatibilityWarning:

  This is a future version incompatibility warning from Holidays v0.62
  to inform you about an upcoming change in our API versioning strategy that may affect your
  project's dependencies. Starting from version 1.0 onwards, we will be following a loose form of
  Semantic Versioning (SemVer, https://semver.org) to provide clearer communication regarding any
  potential breaking changes.

  This means that while we strive to maintain backward compatibility, there might be occasional
  updates that introduce breaking changes to our API. To ensure the stability of your projects,
  we highly recommend pinning the version of our API that you rely on. You can pin your current
  holidays v0.x dependency (e.g., holidays==0.62) or limit it (e.g., holidays<1.0) in order to
  avoid potentially unwanted upgrade to the version 1.0 when it's released (ETA 2025Q1-Q2).

  If you have any questions or concerns regarding this change, please don't hesitate to reach out
  to us via https://github.com/vacanza/holidays/discussions/1800.

    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html

---------- coverage: platform win32, python 3.10.11-final-0 ----------
Name                                                             Stmts   Miss  Cover   Missing
----------------------------------------------------------------------------------------------
src\__init__.py                                                      0      0   100%
src\abnamro_bsrc_etl\__init__.py                                     0      0   100%
src\abnamro_bsrc_etl\config\__init__.py                              0      0   100%
src\abnamro_bsrc_etl\config\business_logic.py                       54      0   100%
src\abnamro_bsrc_etl\config\constants.py                             6      1    83%   5
src\abnamro_bsrc_etl\config\exceptions.py                           37      0   100%
src\abnamro_bsrc_etl\config\process.py                               7      0   100%
src\abnamro_bsrc_etl\config\schema.py                                5      0   100%
src\abnamro_bsrc_etl\config\ssf_tables.py                            2      0   100%
src\abnamro_bsrc_etl\dq\__init__.py                                  0      0   100%
src\abnamro_bsrc_etl\dq\dq_validation.py                           153     83    46%   69, 85, 178-193, 213-261, 281-298, 335-362, 389-404, 429-498
src\abnamro_bsrc_etl\extract\__init__.py                             0      0   100%
src\abnamro_bsrc_etl\extract\master_data_sql.py                     96     16    83%   87, 116-136, 162-164
src\abnamro_bsrc_etl\month_setup\__init__.py                         0      0   100%
src\abnamro_bsrc_etl\month_setup\dial_derive_snapshotdate.py        36      0   100%
src\abnamro_bsrc_etl\month_setup\metadata_log_tables.py             40      9    78%   175-180, 187-191, 197-201
src\abnamro_bsrc_etl\month_setup\setup_new_month.py                 29     11    62%   60-84
src\abnamro_bsrc_etl\scripts\__init__.py                             0      0   100%
src\abnamro_bsrc_etl\scripts\check_dependencies.py                  26      0   100%
src\abnamro_bsrc_etl\scripts\dial_check_delayed_files.py            27      0   100%
src\abnamro_bsrc_etl\scripts\dial_staging_process.py                65      0   100%
src\abnamro_bsrc_etl\scripts\export_tine_tables.py                  33      0   100%
src\abnamro_bsrc_etl\scripts\new_month_setup.py                      7      0   100%
src\abnamro_bsrc_etl\scripts\nonssf_staging_process.py              63      0   100%
src\abnamro_bsrc_etl\scripts\run_mapping.py                         26      0   100%
src\abnamro_bsrc_etl\scripts\ssf_staging_process.py                 58      0   100%
src\abnamro_bsrc_etl\scripts\ssf_staging_process_xml.py             24      0   100%
src\abnamro_bsrc_etl\staging\__init__.py                             0      0   100%
src\abnamro_bsrc_etl\staging\extract_base.py                        77      3    96%   162-164
src\abnamro_bsrc_etl\staging\extract_dial_data.py                   77      0   100%
src\abnamro_bsrc_etl\staging\extract_nonssf_data.py                153     17    89%   90, 121-126, 188-192, 195-200, 224-225, 274, 355-357, 419-423
src\abnamro_bsrc_etl\staging\extract_ssf_data.py                   179     17    91%   271-275, 301-307, 322-323, 354, 461-462, 514-515, 518-521, 616-619
src\abnamro_bsrc_etl\staging\status.py                              58      3    95%   18, 53-54
src\abnamro_bsrc_etl\transform\__init__.py                           0      0   100%
src\abnamro_bsrc_etl\transform\complex_types.py                     15      0   100%
src\abnamro_bsrc_etl\transform\table_write_and_comment.py           79      0   100%
src\abnamro_bsrc_etl\transform\transform_business_logic_sql.py       9      0   100%
src\abnamro_bsrc_etl\utils\__init__.py                               0      0   100%
src\abnamro_bsrc_etl\utils\alias_util.py                            18      0   100%
src\abnamro_bsrc_etl\utils\azure_utils.py                            5      0   100%
src\abnamro_bsrc_etl\utils\export_parquet.py                        22      2    91%   67-68
src\abnamro_bsrc_etl\utils\get_dbutils.py                            6      1    83%   11
src\abnamro_bsrc_etl\utils\get_env.py                               12      0   100%
src\abnamro_bsrc_etl\utils\logging_util.py                          10      0   100%
src\abnamro_bsrc_etl\utils\parameter_utils.py                       25      0   100%
src\abnamro_bsrc_etl\utils\parse_yaml.py                            28     18    36%   33-66, 91, 119-127
src\abnamro_bsrc_etl\utils\sources_util.py                          56      7    88%   106-110, 149, 193, 202
src\abnamro_bsrc_etl\utils\table_logging.py                         19      0   100%
src\abnamro_bsrc_etl\utils\table_schema.py                           6      1    83%   16
src\abnamro_bsrc_etl\utils\transformations_util.py                  20      0   100%
src\abnamro_bsrc_etl\utils\xml_utils.py                             86      5    94%   101-116
src\abnamro_bsrc_etl\validate\__init__.py                            0      0   100%
src\abnamro_bsrc_etl\validate\base.py                                5      0   100%
src\abnamro_bsrc_etl\validate\expressions.py                        34     12    65%   37-61
src\abnamro_bsrc_etl\validate\run_all.py                            15      0   100%
src\abnamro_bsrc_etl\validate\sources.py                            33      0   100%
src\abnamro_bsrc_etl\validate\transformations.py                   200     36    82%   66-72, 120, 135-136, 307-317, 327-354, 477-478
src\abnamro_bsrc_etl\validate\validate_sql.py                       63      1    98%   87
src\abnamro_bsrc_etl\validate\yaml.py                               19      0   100%
----------------------------------------------------------------------------------------------
TOTAL                                                             2123    243    89%
Coverage HTML written to dir htmlcov

=================================================================================== short test summary info ====================================================================================
FAILED test/test_dq/test_dq_validation.py::test_dq_validation[dq_test_happy-True-Checks completed successfully] - assert None == True
FAILED test/test_dq/test_dq_validation.py::test_dq_validation[dq_test_happy_col_num-True-Checks completed successfully] - assert None == True
FAILED test/test_dq/test_dq_validation.py::test_dq_validation[dq_test_unhappy_col_num-False-Checks completed - DQ issues found] - assert None == False
FAILED test/test_dq/test_dq_validation.py::test_dq_validation[dq_test_unhappy_pk_dup-False-Checks completed - DQ issues found] - assert None == False
FAILED test/test_dq/test_dq_validation.py::test_dq_validation[dq_test_unhappy_pk_null-False-Checks completed - DQ issues found] - assert None == False
FAILED test/test_dq/test_dq_validation.py::test_dq_validation[dq_test_unhappy_not_null-False-Checks completed - DQ issues found] - assert None == False
FAILED test/test_dq/test_dq_validation.py::test_dq_validation[dq_test_unhappy_num_cols-False-Checks completed - DQ issues found] - assert None == False
FAILED test/test_dq/test_dq_validation.py::test_dq_validation[dq_test_unhappy_type_cols-False-Checks completed - DQ issues found] - assert None == False
FAILED test/test_dq/test_dq_validation.py::test_dq_validation[dq_test_unhappy_ref-False-Checks completed - DQ issues found] - assert None == False
FAILED test/test_dq/test_dq_validation.py::test_dq_validation[dq_test_unhappy_ref_filter-False-Checks completed - DQ issues found] - assert None == False
FAILED test/test_dq/test_dq_validation.py::test_dq_validation[dq_test_unhappy_unique-False-Checks completed - DQ issues found] - assert None == False
FAILED test/test_dq/test_dq_validation.py::test_columns[dq_test_happy-True-expected_logging0] - AssertionError: assert None == True
FAILED test/test_dq/test_dq_validation.py::test_columns[dq_test_happy_col_num-True-expected_logging1] - AssertionError: assert None == True
FAILED test/test_dq/test_dq_validation.py::test_columns[dq_test_unhappy_col_num-False-expected_logging3] - AssertionError: assert None == False
FAILED test/test_dq/test_dq_validation.py::test_columns[dq_test_unhappy_num_cols-False-expected_logging4] - AssertionError: assert None == False
FAILED test/test_dq/test_dq_validation.py::test_columns[dq_test_unhappy_type_cols-False-expected_logging5] - AssertionError: assert None == False
FAILED test/test_dq/test_dq_validation.py::test_primary_key[dq_test_happy-True-expected_logging0] - AssertionError: assert None == True
FAILED test/test_dq/test_dq_validation.py::test_primary_key[dq_test_unhappy_pk_dup-False-expected_logging2] - AssertionError: assert None == False
FAILED test/test_dq/test_dq_validation.py::test_primary_key[dq_test_unhappy_pk_null-False-expected_logging3] - AssertionError: assert None == False
FAILED test/test_dq/test_dq_validation.py::test_not_nulls[dq_test_happy-True-expected_logging0] - AssertionError: assert None == True
FAILED test/test_dq/test_dq_validation.py::test_not_nulls[dq_test_unhappy_not_null-False-expected_logging2] - AssertionError: assert None == False
FAILED test/test_dq/test_dq_validation.py::test_referential_integrity[dq_test_happy-True-expected_logging0] - AssertionError: assert None == True
FAILED test/test_dq/test_dq_validation.py::test_referential_integrity[dq_test_unhappy_ref-False-expected_logging2] - AssertionError: assert None == False
FAILED test/test_dq/test_dq_validation.py::test_referential_integrity[dq_test_unhappy_ref_filter-False-expected_logging3] - AssertionError: assert None == False
FAILED test/test_dq/test_dq_validation.py::test_unique[dq_test_happy-True-expected_logging0] - AssertionError: assert None == True
FAILED test/test_dq/test_dq_validation.py::test_unique[dq_test_unhappy_unique-False-expected_logging2] - AssertionError: assert None == False
FAILED test/test_dq/test_dq_validation.py::test_generic_checks[dq_test_fr_happy-fr-log_range0] - assert None
FAILED test/test_dq/test_dq_validation.py::test_generic_checks[dq_test_fr_no_generic-fr-log_range1] - assert None
FAILED test/test_dq/test_dq_validation.py::test_generic_checks[dq_test_nospecific_happy-nospecific-log_range2] - assert None
FAILED test/test_transform/test_pipeline_yaml_integrated_target.py::test_pipeline_yaml_integrated_target[parameters0-test_catalog.test_schema_20240801.testd1_test_target_table] - FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\B25712\\bsrc-etl-venv\\bsrc-etl\\test_data\\business_logic\\..\\test\\data\\TEST_YAML.yml'
FAILED test/test_transform/test_pipeline_yaml_integrated_target.py::test_pipeline_yaml_pivot - FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\B25712\\bsrc-etl-venv\\bsrc-etl\\test_data\\business_logic\\..\\test\\data\\TEST_YAML_PIVOT.yml'
FAILED test/test_transform/test_pipeline_yaml_integrated_target.py::test_run_mapping - FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\B25712\\bsrc-etl-venv\\bsrc-etl\\test_data\\test\\data\\TEST_YAML_PIVOT.yml'
FAILED test/test_utils/test_parse_yaml.py::test_parse_yaml[IHUB-FR1-202412-output0] - FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\B25712\\bsrc-etl-venv\\bsrc-etl\\test_data\\business_logic\\..\\test\\data\\test_parse_yaml.yml'
FAILED test/test_utils/test_table_schema.py::test_get_schema - FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\B25712\\bsrc-etl-venv\\bsrc-etl\\test_data\\table_structures\\..\\test\\data\\TEST_SCHEMA.json'
==================================================================== 34 failed, 363 passed, 1 warning in 748.82s (0:12:28) ===================================================================== 
SUCCESS: The process with PID 15476 (child process of PID 17844) has been terminated.
SUCCESS: The process with PID 17844 (child process of PID 23064) has been terminated.
SUCCESS: The process with PID 23064 (child process of PID 23084) has been terminated.
