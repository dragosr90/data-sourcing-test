(bsrc-etl-venv) PS C:\Users\B25712\bsrc-etl-venv\bsrc-etl> pytest test/staging/test_extract_non_ssf_data.py
========================================================================================== test session starts ===========================================================================================
platform win32 -- Python 3.10.11, pytest-8.3.3, pluggy-1.5.0
rootdir: C:\Users\B25712\bsrc-etl-venv\bsrc-etl
configfile: pyproject.toml
plugins: cov-6.0.0, mock-3.14.0
collected 26 items

test\staging\test_extract_non_ssf_data.py ........F.....F.........FF                                                                                                                                [100%]

================================================================================================ FAILURES ================================================================================================ 
__________________________________________________________________ test_place_static_data_with_redelivery_status[202503-test-container] __________________________________________________________________ 

self = <MagicMock name='cp' id='1781549518944'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'cp' to have been called once. Called 0 times.

C:\Program Files\Python310\lib\unittest\mock.py:908: AssertionError

During handling of the above exception, another exception occurred:

spark_session = <pyspark.sql.session.SparkSession object at 0x0000019ECC8D8490>, mocker = <pytest_mock.plugin.MockerFixture object at 0x0000019ECC912740>, run_month = '202503'
source_container = 'test-container', caplog = <_pytest.logging.LogCaptureFixture object at 0x0000019ECC93CFA0>

    @pytest.mark.parametrize(
        ("run_month", "source_container"),
        [("202503", "test-container")],
    )
    def test_place_static_data_with_redelivery_status(
        spark_session,
        mocker,
        run_month,
        source_container,
        caplog,
    ):
        """Test place_static_data processes files with REDELIVERY status."""
        test_container = f"abfss://{source_container}@bsrcdadls.dfs.core.windows.net"

        # Create mock metadata DataFrame with REDELIVERY status
        schema_meta = [
            "SourceSystem",
            "SourceFileName",
            "SourceFileFormat",
            "SourceFileDelimiter",
            "StgTableName",
            "FileDeliveryStep",
            "FileDeliveryStatus",
        ]
        mock_meta = spark_session.createDataFrame(
            [
                (
                    "lrd_static",
                    "TEST_FILE",
                    ".txt",
                    "|",
                    "test_file",
                    10,  # REDELIVERY status
                    "Expected",
                ),
            ],
            schema=schema_meta,
        )

        schema_log = StructType(
            [
                StructField("SourceSystem", StringType(), True),  # noqa: FBT003
                StructField("SourceFileName", StringType(), True),  # noqa: FBT003
                StructField("DeliveryNumber", IntegerType(), True),  # noqa: FBT003
                StructField("FileDeliveryStep", IntegerType(), True),  # noqa: FBT003
                StructField("FileDeliveryStatus", StringType(), True),  # noqa: FBT003
                StructField("Result", StringType(), True),  # noqa: FBT003
                StructField("LastUpdatedDateTimestamp", TimestampType(), True),  # noqa: FBT003
                StructField("Comment", StringType(), True),  # noqa: FBT003
            ]
        )
        mock_log = spark_session.createDataFrame([], schema=schema_log)

        mock_read = mocker.patch("pyspark.sql.SparkSession.read", autospec=True)
        mock_read.table.side_effect = [mock_meta, mock_log]

        extraction = ExtractNonSSFData(
            spark_session,
            run_month,
            source_container=source_container,
        )

        # Mock filesystem operations
        mock_dbutils_fs_ls = mocker.patch.object(extraction.dbutils.fs, "ls")
        mock_dbutils_fs_ls.side_effect = [
            [
                FileInfoMock(
                    {
                        "path": f"{test_container}/LRD_STATIC/processed/TEST_FILE_20240101.txt",  # noqa: E501
                        "name": "TEST_FILE_20240101.txt",
                    }
                )
            ],
            [
                FileInfoMock(
                    {
                        "path": f"{test_container}/LRD_STATIC/processed/TEST_FILE_20240101.txt",  # noqa: E501
                        "name": "TEST_FILE_20240101.txt",
                    }
                )
            ],
        ]

        mock_dbutils_fs_cp = mocker.patch.object(extraction.dbutils.fs, "cp")
        mock_save_table = mocker.patch("pyspark.sql.DataFrameWriter.saveAsTable")

        # Call place_static_data with deadline passed - should process REDELIVERY files
        result = extraction.place_static_data([], deadline_passed=True)

        # Verify that the file was copied
>       mock_dbutils_fs_cp.assert_called_once()
E       AssertionError: Expected 'cp' to have been called once. Called 0 times.

test\staging\test_extract_non_ssf_data.py:951: AssertionError
------------------------------------------------------------------------------------------ Captured stdout call ------------------------------------------------------------------------------------------ 
2025-07-09 11:57:53 [INFO] place_static_data:  File TEST_FILE is not in expected status. Skipping copy from processed folder.
2025-07-09 11:57:53 [INFO] place_static_data:  File TEST_FILE is not in expected status. Skipping copy from processed folder.
------------------------------------------------------------------------------------------ Captured stderr call ------------------------------------------------------------------------------------------ 

------------------------------------------------------------------------------------------- Captured log call -------------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:extract_nonssf_data.py:132 File TEST_FILE is not in expected status. Skipping copy from processed folder.
___________________________________________________________________ test_convert_to_parquet_unsupported_format[202503-test-container] ____________________________________________________________________

self = <MagicMock name='update_log_metadata' id='1781549832304'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'update_log_metadata' to have been called once. Called 0 times.

C:\Program Files\Python310\lib\unittest\mock.py:908: AssertionError

During handling of the above exception, another exception occurred:

spark_session = <pyspark.sql.session.SparkSession object at 0x0000019ECC8D8490>, mocker = <pytest_mock.plugin.MockerFixture object at 0x0000019ECCA74670>, run_month = '202503'
source_container = 'test-container', caplog = <_pytest.logging.LogCaptureFixture object at 0x0000019ECCB0F760>

    @pytest.mark.parametrize(
        ("run_month", "source_container"),
        [("202503", "test-container")],
    )
    def test_convert_to_parquet_unsupported_format(
        spark_session,
        mocker,
        run_month,
        source_container,
        caplog,
    ):
        """Test convert_to_parquet with unsupported file format."""
        # Create mock metadata DataFrame
        schema_meta = [
            "SourceSystem",
            "SourceFileName",
            "SourceFileFormat",
            "SourceFileDelimiter",
            "StgTableName",
            "FileDeliveryStep",
            "FileDeliveryStatus",
        ]
        mock_meta = spark_session.createDataFrame(
            [
                (
                    "nme",
                    "TEST_FILE",
                    ".json",  # Unsupported format
                    ",",
                    "test_file",
                    0,
                    "Expected",
                ),
            ],
            schema=schema_meta,
        )

        # Create empty log DataFrame
        schema_log = StructType(
            [
                StructField("SourceSystem", StringType(), True),  # noqa: FBT003
                StructField("SourceFileName", StringType(), True),  # noqa: FBT003
                StructField("DeliveryNumber", IntegerType(), True),  # noqa: FBT003
                StructField("FileDeliveryStep", IntegerType(), True),  # noqa: FBT003
                StructField("FileDeliveryStatus", StringType(), True),  # noqa: FBT003
                StructField("Result", StringType(), True),  # noqa: FBT003
                StructField("LastUpdatedDateTimestamp", TimestampType(), True),  # noqa: FBT003
                StructField("Comment", StringType(), True),  # noqa: FBT003
            ]
        )
        mock_log = spark_session.createDataFrame([], schema=schema_log)

        # Mock spark.read
        mock_read = mocker.patch("pyspark.sql.SparkSession.read", autospec=True)
        mock_read.table.side_effect = [mock_meta, mock_log]

        extraction = ExtractNonSSFData(
            spark_session,
            run_month,
            source_container=source_container,
        )

        # Mock the update_log_metadata method to verify it's called
        mock_update = mocker.patch.object(extraction, "update_log_metadata")

        # Test convert_to_parquet
        result = extraction.convert_to_parquet("NME", "TEST_FILE.json")
        assert result is False
        assert "Unsupported file format: .json" in caplog.text
        # Verify that update_log_metadata was called with FAILURE
>       mock_update.assert_called_once()
E       AssertionError: Expected 'update_log_metadata' to have been called once. Called 0 times.

test\staging\test_extract_non_ssf_data.py:1488: AssertionError
------------------------------------------------------------------------------------------ Captured stdout call ------------------------------------------------------------------------------------------
2025-07-09 11:58:18 [ERROR] convert_to_parquet:  Unsupported file format: .json. Only .csv, .txt and .parquet are supported.
2025-07-09 11:58:18 [ERROR] convert_to_parquet:  Unsupported file format: .json. Only .csv, .txt and .parquet are supported.
------------------------------------------------------------------------------------------ Captured stderr call ------------------------------------------------------------------------------------------ 

------------------------------------------------------------------------------------------- Captured log call -------------------------------------------------------------------------------------------- 
ERROR    betl_src_poc_logger:extract_nonssf_data.py:427 Unsupported file format: .json. Only .csv, .txt and .parquet are supported.
_________________________________________________________________________ test_save_to_stg_table_failure[202503-test-container] __________________________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x0000019ECC8D8490>, mocker = <pytest_mock.plugin.MockerFixture object at 0x0000019ECC93D750>, run_month = '202503'
source_container = 'test-container'

    @pytest.mark.parametrize(
        ("run_month", "source_container"),
        [("202503", "test-container")],
    )
    def test_save_to_stg_table_failure(
        spark_session,
        mocker,
        run_month,
        source_container,
    ):
        """Test save_to_stg_table failure handling."""
        # Create mock metadata DataFrame
        schema_meta = [
            "SourceSystem",
            "SourceFileName",
            "SourceFileFormat",
            "SourceFileDelimiter",
            "StgTableName",
            "FileDeliveryStep",
            "FileDeliveryStatus",
        ]
        mock_meta = spark_session.createDataFrame(
            [
                (
                    "nme",
                    "TEST_FILE",
                    ".csv",
                    ",",
                    "test_file",
                    0,
                    "Expected",
                ),
            ],
            schema=schema_meta,
        )

        schema_log = StructType(
            [
                StructField("SourceSystem", StringType(), True),  # noqa: FBT003
                StructField("SourceFileName", StringType(), True),  # noqa: FBT003
                StructField("DeliveryNumber", IntegerType(), True),  # noqa: FBT003
                StructField("FileDeliveryStep", IntegerType(), True),  # noqa: FBT003
                StructField("FileDeliveryStatus", StringType(), True),  # noqa: FBT003
                StructField("Result", StringType(), True),  # noqa: FBT003
                StructField("LastUpdatedDateTimestamp", TimestampType(), True),  # noqa: FBT003
                StructField("Comment", StringType(), True),  # noqa: FBT003
            ]
        )
        mock_log = spark_session.createDataFrame([], schema=schema_log)

        mock_read = mocker.patch("pyspark.sql.SparkSession.read", autospec=True)
        mock_read.table.side_effect = [mock_meta, mock_log]

        extraction = ExtractNonSSFData(
            spark_session,
            run_month,
            source_container=source_container,
        )
    
        # Create a dummy DataFrame
        dummy_df = spark_session.createDataFrame([(1, "test")], ["id", "value"])

        # Mock the saveAsTable to fail by patching at the method level
        mock_save_table = mocker.patch("pyspark.sql.DataFrameWriter.saveAsTable")
        mock_save_table.side_effect = Exception("Write failed")

        # Test save_to_stg_table with failure
>       result = extraction.save_to_stg_table(
            data=dummy_df,
            stg_table_name="test_table",
            source_system="NME",
            file_name="TEST_FILE.csv",
        )

test\staging\test_extract_non_ssf_data.py:2264:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  
src\staging\extract_base.py:344: in save_to_stg_table
    comment = self.write_table_with_exception(data, full_path)
src\staging\extract_base.py:109: in write_table_with_exception
    data.write.mode("overwrite").saveAsTable(full_table_name)
C:\Program Files\Python310\lib\unittest\mock.py:1114: in __call__
    return self._mock_call(*args, **kwargs)
C:\Program Files\Python310\lib\unittest\mock.py:1118: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  

self = <MagicMock name='saveAsTable' id='1781550038848'>, args = ('bsrc_d.stg_202503.test_table',), kwargs = {}, effect = Exception('Write failed')

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method

        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
>               raise effect
E               Exception: Write failed

C:\Program Files\Python310\lib\unittest\mock.py:1173: Exception
_______________________________________________________________________ test_validate_data_quality_failure[202503-test-container] ________________________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x0000019ECC8D8490>, mocker = <pytest_mock.plugin.MockerFixture object at 0x0000019ECCB1E470>, run_month = '202503'
source_container = 'test-container'

    @pytest.mark.parametrize(
        ("run_month", "source_container"),
        [("202503", "test-container")],
    )
    def test_validate_data_quality_failure(
        spark_session,
        mocker,
        run_month,
        source_container,
    ):
        """Test validate_data_quality failure handling."""
        # Create mock metadata DataFrame
        schema_meta = [
            "SourceSystem",
            "SourceFileName",
            "SourceFileFormat",
            "SourceFileDelimiter",
            "StgTableName",
            "FileDeliveryStep",
            "FileDeliveryStatus",
        ]
        mock_meta = spark_session.createDataFrame(
            [
                (
                    "nme",
                    "TEST_FILE",
                    ".csv",
                    ",",
                    "test_file",
                    0,
                    "Expected",
                ),
            ],
            schema=schema_meta,
        )

        schema_log = StructType(
            [
                StructField("SourceSystem", StringType(), True),  # noqa: FBT003
                StructField("SourceFileName", StringType(), True),  # noqa: FBT003
                StructField("DeliveryNumber", IntegerType(), True),  # noqa: FBT003
                StructField("FileDeliveryStep", IntegerType(), True),  # noqa: FBT003
                StructField("FileDeliveryStatus", StringType(), True),  # noqa: FBT003
                StructField("Result", StringType(), True),  # noqa: FBT003
                StructField("LastUpdatedDateTimestamp", TimestampType(), True),  # noqa: FBT003
                StructField("Comment", StringType(), True),  # noqa: FBT003
            ]
        )
        mock_log = spark_session.createDataFrame([], schema=schema_log)

        mock_read = mocker.patch("pyspark.sql.SparkSession.read", autospec=True)
        mock_read.table.side_effect = [mock_meta, mock_log]

        extraction = ExtractNonSSFData(
            spark_session,
            run_month,
            source_container=source_container,
        )

        # Mock DQValidation to raise exception
        mock_dq_validation = mocker.patch("src.staging.extract_base.DQValidation")
        mock_dq_validation.side_effect = Exception("Table not found")

        # Test validate_data_quality with failure
>       result = extraction.validate_data_quality(
            source_system="NME",
            file_name="TEST_FILE.csv",
            stg_table_name="test_file",
        )

test\staging\test_extract_non_ssf_data.py:2337:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  
src\staging\extract_base.py:386: in validate_data_quality
    result = DQValidation(
C:\Program Files\Python310\lib\unittest\mock.py:1114: in __call__
    return self._mock_call(*args, **kwargs)
C:\Program Files\Python310\lib\unittest\mock.py:1118: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  

self = <MagicMock name='DQValidation' id='1781550557952'>, args = (<pyspark.sql.session.SparkSession object at 0x0000019ECC8D8490>,)
kwargs = {'dq_check_folder': 'dq_checks', 'run_month': '202503', 'schema_name': 'stg', 'source_system': 'nme', ...}, effect = Exception('Table not found')

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method

        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
>               raise effect
E               Exception: Table not found

C:\Program Files\Python310\lib\unittest\mock.py:1173: Exception
------------------------------------------------------------------------------------------ Captured stdout call ------------------------------------------------------------------------------------------ 
2025-07-09 11:58:45 [INFO] standardize_delivery_entity:  Standardized delivery entity 'NME' to 'nme'
2025-07-09 11:58:45 [INFO] standardize_delivery_entity:  Standardized delivery entity 'NME' to 'nme'
------------------------------------------------------------------------------------------- Captured log call -------------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:parameter_utils.py:49 Standardized delivery entity 'NME' to 'nme'
============================================================================================ warnings summary ============================================================================================ 
..\bsrc-etl-venv\lib\site-packages\holidays\deprecations\v1_incompatibility.py:40
  C:\Users\B25712\bsrc-etl-venv\bsrc-etl-venv\lib\site-packages\holidays\deprecations\v1_incompatibility.py:40: FutureIncompatibilityWarning:

  This is a future version incompatibility warning from Holidays v0.62
  to inform you about an upcoming change in our API versioning strategy that may affect your
  project's dependencies. Starting from version 1.0 onwards, we will be following a loose form of
  Semantic Versioning (SemVer, https://semver.org) to provide clearer communication regarding any
  potential breaking changes.

  This means that while we strive to maintain backward compatibility, there might be occasional
  updates that introduce breaking changes to our API. To ensure the stability of your projects,
  we highly recommend pinning the version of our API that you rely on. You can pin your current
  holidays v0.x dependency (e.g., holidays==0.62) or limit it (e.g., holidays<1.0) in order to
  avoid potentially unwanted upgrade to the version 1.0 when it's released (ETA 2025Q1-Q2).

  If you have any questions or concerns regarding this change, please don't hesitate to reach out
  to us via https://github.com/vacanza/holidays/discussions/1800.

    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html

---------- coverage: platform win32, python 3.10.11-final-0 ----------
Name                                            Stmts   Miss  Cover   Missing
-----------------------------------------------------------------------------
src\__init__.py                                     0      0   100%
src\config\__init__.py                              0      0   100%
src\config\business_logic.py                       52      0   100%
src\config\constants.py                             1      0   100%
src\config\exceptions.py                           21      0   100%
src\config\process.py                               4      4     0%   7-10
src\config\schema.py                                4      4     0%   3-52
src\dq\__init__.py                                  0      0   100%
src\dq\dq_validation.py                           141     88    38%   54-55, 69, 77-78, 88, 176-181, 201-249, 269-286, 323-350, 377-392, 414-486
src\extract\__init__.py                             0      0   100%
src\extract\master_data_sql.py                     90     71    21%   31-33, 37-38, 42, 59-88, 109-137, 149-156, 162-164, 188, 222-231, 285-300, 329-356
src\month_setup\__init__.py                         0      0   100%
src\month_setup\dial_derive_snapshotdate.py        32     27    16%   12-18, 28-37, 47-50, 69-85
src\month_setup\metadata_log_tables.py             32     25    22%   23-84, 159, 166-203
src\month_setup\setup_new_month.py                 13     13     0%   17-84
src\staging\__init__.py                             0      0   100%
src\staging\extract_base.py                        65      8    88%   156, 162-164, 304-312
src\staging\extract_dial_data.py                   65     65     0%   16-360
src\staging\extract_nonssf_data.py                135      1    99%   310
src\staging\extract_ssf_data.py                   164    164     0%   26-620
src\staging\status.py                              57      5    91%   18, 53-54, 107, 151
src\transform\__init__.py                           0      0   100%
src\transform\table_write_and_comment.py           72     72     0%   14-237
src\transform\transform_business_logic_sql.py       6      6     0%   6-25
src\utils\__init__.py                               0      0   100%
src\utils\alias_util.py                            13     13     0%   10-109
src\utils\export_parquet.py                        15      6    60%   49-54, 67-68
src\utils\get_dbutils.py                            2      0   100%
src\utils\get_env.py                               10      0   100%
src\utils\logging_util.py                           6      0   100%
src\utils\parameter_utils.py                       23     13    43%   34, 38-42, 79-84, 105-116
src\utils\parse_yaml.py                            22     22     0%   11-127
src\utils\sources_util.py                          52     52     0%   7-218
src\utils\table_logging.py                         14      0   100%
src\utils\table_schema.py                           3      3     0%   8-16
src\utils\transformations_util.py                  17     12    29%   20-25, 39, 51, 65-68
src\validate\__init__.py                            0      0   100%
src\validate\base.py                                4      4     0%   4-7
src\validate\expressions.py                        27     27     0%   15-75
src\validate\run_all.py                             7      7     0%   12-48
src\validate\sources.py                            29     29     0%   7-67
src\validate\transformations.py                   192    192     0%   21-593
src\validate\validate_sql.py                       54     54     0%   13-130
src\validate\yaml.py                               18     18     0%   3-34
-----------------------------------------------------------------------------
TOTAL                                            1462   1005    31%
Coverage HTML written to dir htmlcov

======================================================================================== short test summary info ========================================================================================= 
FAILED test/staging/test_extract_non_ssf_data.py::test_place_static_data_with_redelivery_status[202503-test-container] - AssertionError: Expected 'cp' to have been called once. Called 0 times.
FAILED test/staging/test_extract_non_ssf_data.py::test_convert_to_parquet_unsupported_format[202503-test-container] - AssertionError: Expected 'update_log_metadata' to have been called once. Called 0 times.
FAILED test/staging/test_extract_non_ssf_data.py::test_save_to_stg_table_failure[202503-test-container] - Exception: Write failed
FAILED test/staging/test_extract_non_ssf_data.py::test_validate_data_quality_failure[202503-test-container] - Exception: Table not found
========================================================================== 4 failed, 22 passed, 1 warning in 188.61s (0:03:08) =========================================================================== 
(bsrc-etl-venv) PS C:\Users\B25712\bsrc-etl-venv\bsrc-etl> SUCCESS: The process with PID 18336 (child process of PID 11832) has been terminated.
SUCCESS: The process with PID 11832 (child process of PID 18236) has been terminated.
SUCCESS: The process with PID 18236 (child process of PID 20608) has been terminated.
