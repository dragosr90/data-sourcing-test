(bsrc-etl-venv) PS C:\Users\B25712\bsrc-etl-venv\bsrc-etl> pytest test/staging/test_extract_dial_data.py
========================================================================================== test session starts ===========================================================================================
platform win32 -- Python 3.10.11, pytest-8.3.3, pluggy-1.5.0
rootdir: C:\Users\B25712\bsrc-etl-venv\bsrc-etl
configfile: pyproject.toml
plugins: cov-6.0.0, mock-3.14.0
collected 1 item

test\staging\test_extract_dial_data.py F                                                                                                                                                            [100%]

================================================================================================ FAILURES ================================================================================================ 
_____________________________________________________________________________ test_extract_dial_data[202503-test-container] ______________________________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001A0BBD687C0>, mocker = <pytest_mock.plugin.MockerFixture object at 0x000001A0BBD6B2B0>, run_month = '202503'
source_container = 'test-container'

    @pytest.mark.parametrize(
        ("run_month", "source_container"),
        [("202503", "test-container")],
    )
    def test_extract_dial_data(
        spark_session,
        mocker,
        run_month,
        source_container,
    ):
        test_container = f"abfss://{source_container}@bsrcdadls.dfs.core.windows.net"
        month_container = f"abfss://{run_month}@bsrcdadls.dfs.core.windows.net"
        metadata_path = f"bsrc_d.metadata_{run_month}.metadata_dial_schedule"
        log_path = f"bsrc_d.log_{run_month}.log_dial_schedule"

        # Create a mock DataFrame
        schema_trigger = [
            "dial_consumer_name",
            "dial_source_system",
            "dial_source_file_name",
            "dial_source_file_snapshot_datetime",
            "dial_notification_datetime",
            "dial_target_file_full_path",
            "dial_target_file_row_count",
        ]
        mock_trigger = spark_session.createDataFrame(
            [
                ("src", "epr", "TEST_DIAL_V2", "202503", "9999", "path/to/dest2/", 102),
                ("src", "epr", "TEST_DIAL_V3", "202503", "9999", "path/to/dest3/", 103),
                ("src", "epr", "TEST_DIAL_V4", "202509", "9999", "path/to/dest4/", 104),
                ("src", "epr", "TEST_DIAL_V5", "202509", "9999", "path/to/dest5/", 105),
            ],
            schema=schema_trigger,
        )

        schema_meta = [
            "FileDeliveryStatus",
            "FileDeliveryStep",
            "SourceFileName",
            "SnapshotDate",
            "StgTableName",
        ]
        mock_meta = spark_session.createDataFrame(
            [
                ("Extracted", 1, "TEST_DIAL_V1", "202503", "test_dial_v1"),
                ("Expected delivery", 0, "TEST_DIAL_V2", "202503", "test_dial_v2"),
                ("Expected delivery", 0, "TEST_DIAL_V3", "202503", "test_dial_v3"),
            ],
            schema=schema_meta,
        )
        schema_log = [
            "SourceSystem",
            "SourceFileName",
            "SnapshotDate",
            "FileDeliveryStatus",
            "FileDeliveryStep",
            "DeliveryNumber",
            "LastUpdatedDateTimestamp",
            "Comment",
        ]
        mock_log = spark_session.createDataFrame(
            [
                (
                    "epr",
                    "TEST_DIAL_V2",
                    "202503",
                    "Expected delivery",
                    0,
                    0,
                    datetime.now(tz=timezone.utc),
                    "Data file stored in storage location.",
                ),
                (
                    "epr",
                    "TEST_DIAL_V3",
                    "202503",
                    "Expected delivery",
                    0,
                    0,
                    datetime.now(tz=timezone.utc),
                    "Data file stored in storage location.",
                ),
            ],
            schema=schema_log,
        )

        schema_staging_v2 = ["int_col", "str_col"]
        mock_staging_v2 = spark_session.createDataFrame(
            data=[(3, "b")], schema=schema_staging_v2
        )

        schema_staging_v3 = ["dt_col", "bool_col"]
        mock_staging_v3 = spark_session.createDataFrame(
            data=[(datetime.now(tz=timezone.utc), True)], schema=schema_staging_v3
        )
        mock_data_path_2 = spark_session.createDataFrame(
            [(x,) for x in range(102)],
            schema=StructType([StructField("col02", IntegerType())]),
        )
        mock_data_path_3 = spark_session.createDataFrame(
            [(x,) for x in range(103)],
            schema=StructType([StructField("col03", IntegerType())]),
        )

        # Mock spark.read.json and spark.read.table to return the mock DataFrames
        mock_read = mocker.patch("pyspark.sql.SparkSession.read", autospec=True)
        mock_read.json.return_value = mock_trigger
        mock_read.table.side_effect = [
            mock_meta,
            mock_log,
            mock_staging_v2,
            mock_staging_v3,
        ]

        # Check ExtractDialData class initialisation
        extraction = ExtractDialData(
            spark_session,
            run_month,
            source_container=source_container,
        )

        mock_read.json.assert_called_once_with(
            f"{test_container}/dial_trigger",
            schema=StructType(
                [
                    StructField("dial_source_file_name", StringType()),
                    StructField("dial_source_system", StringType()),
                    StructField("dial_source_file_snapshot_datetime", StringType()),
                    StructField("dial_notification_datetime", StringType()),
                    StructField("dial_consumer_name", StringType()),
                    StructField("dial_target_file_full_path", StringType()),
                    StructField("dial_target_file_row_count", StringType()),
                ]
            ),
        )
        # Verify that spark.read.table was called with the correct arguments
        mock_read.table.assert_any_call("bsrc_d.metadata_202503.metadata_dial_schedule")
        mock_read.table.assert_any_call("bsrc_d.log_202503.log_dial_schedule")

        assert extraction.trigger_data == mock_trigger

        matching_meta_trigger_data = extraction.get_matching_meta_trigger_data()
        triggers, triggers_archive = (
            extraction.get_triggers(matching_meta_trigger_data, matching=matching)
            for matching in [True, False]
        )
        assert matching_meta_trigger_data.columns == [
            *schema_trigger,
            *schema_meta,
            "trigger_file_name",
        ]
        assert matching_meta_trigger_data.filter("SourceFileName is not null").count() == 2
        assert matching_meta_trigger_data.filter("SourceFileName is null").count() == 2

        # Create a mock dbutils
        mock_dbutils = mocker.patch.object(
            extraction, "dbutils", new_callable=mocker.MagicMock()
        )

        # Mock for reading parquet files
        mock_read.parquet.side_effect = [mock_data_path_2, mock_data_path_3]

        # Mock for writing parquet files
        mock_write = mocker.MagicMock()
        mock_mode = mocker.MagicMock()
        mock_write.mode.return_value = mock_mode
        mock_mode.parquet.return_value = None
        mocker.patch.object(
            DataFrame,
            "write",
            new_callable=mocker.PropertyMock,
            return_value=mock_write,
        )

        # Run full flow
        for trigger_file_name, trigger_config in triggers.items():
            file_mapping = extraction.get_file_mapping(
                matching_meta_trigger_data.filter("SourceFileName is not null")
            )[trigger_config["file_name"]]
            data_path, row_count, stg_table_name = tuple(file_mapping.values())

            # 1. Extracted raw parquet data
            data = extraction.extract_from_blob_storage(
                data_path=data_path,
                **trigger_config,
            )

            # 2. Validated expected count
            result_row_count = extraction.validate_expected_row_count(
                data=data,
                expected_row_count=row_count,
                **trigger_config,
            )

            # 2. Validated expected count - Unhappy
            result_row_count_unhappy = extraction.validate_expected_row_count(
                data=data,
                expected_row_count=+1,
                **trigger_config,
            )

            # 3. Copy parquet file
            result_copy_to_blob = extraction.copy_to_blob_storage(
                data=data,
                **trigger_config,
            )

            # 4. Moved JSON trigger file
            extraction.move_trigger_file(
                trigger_file_name=trigger_file_name,
                **trigger_config,
            )

            # 5. Loaded Staging table
            # Filter trigger_config to remove 'source_system' key
            # that save_to_stg_table doesn't accept
            stg_table_params = {k: v for k, v in trigger_config.items() if k != "source_system"} # noqa: E501
>           result_staging = extraction.save_to_stg_table(
                data=data,
                stg_table_name=stg_table_name,
                **stg_table_params,
            )
E           TypeError: ExtractStagingData.save_to_stg_table() got an unexpected keyword argument 'snapshot_date'

test\staging\test_extract_dial_data.py:228: TypeError
----------------------------------------------------------------------------------------- Captured stderr setup ------------------------------------------------------------------------------------------ 
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
------------------------------------------------------------------------------------------ Captured stdout call ------------------------------------------------------------------------------------------ 
2025-07-14 16:17:16 [INFO] update_log_metadata:  FileDeliveryStatus: Extracted raw parquet data for TEST_DIAL_V2
2025-07-14 16:17:16 [INFO] update_log_metadata:  FileDeliveryStatus: Extracted raw parquet data for TEST_DIAL_V2
2025-07-14 16:17:21 [INFO] update_log_metadata:  FileDeliveryStatus: Validated expected count for TEST_DIAL_V2
2025-07-14 16:17:21 [INFO] update_log_metadata:  FileDeliveryStatus: Validated expected count for TEST_DIAL_V2
2025-07-14 16:17:26 [ERROR] validate_expected_row_count:  Expected 1, received 102
2025-07-14 16:17:26 [ERROR] validate_expected_row_count:  Expected 1, received 102
2025-07-14 16:17:26 [INFO] update_log_metadata:  FileDeliveryStatus: Validated expected count for TEST_DIAL_V2
2025-07-14 16:17:26 [INFO] update_log_metadata:  FileDeliveryStatus: Validated expected count for TEST_DIAL_V2
2025-07-14 16:17:29 [INFO] export_to_parquet:  Exported to sourcing_landing_data/DIAL/epr/TEST_DIAL_V2.parquet
2025-07-14 16:17:29 [INFO] export_to_parquet:  Exported to sourcing_landing_data/DIAL/epr/TEST_DIAL_V2.parquet
2025-07-14 16:17:29 [INFO] update_log_metadata:  FileDeliveryStatus: Copied parquet to storage account for TEST_DIAL_V2
2025-07-14 16:17:29 [INFO] update_log_metadata:  FileDeliveryStatus: Copied parquet to storage account for TEST_DIAL_V2
2025-07-14 16:17:32 [INFO] update_log_metadata:  FileDeliveryStatus: Moved JSON trigger file for TEST_DIAL_V2
2025-07-14 16:17:32 [INFO] update_log_metadata:  FileDeliveryStatus: Moved JSON trigger file for TEST_DIAL_V2
------------------------------------------------------------------------------------------ Captured stderr call ------------------------------------------------------------------------------------------ 

------------------------------------------------------------------------------------------- Captured log call -------------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:extract_base.py:214 FileDeliveryStatus: Extracted raw parquet data for TEST_DIAL_V2
INFO     betl_src_poc_logger:extract_base.py:214 FileDeliveryStatus: Validated expected count for TEST_DIAL_V2
ERROR    betl_src_poc_logger:extract_dial_data.py:221 Expected 1, received 102
INFO     betl_src_poc_logger:extract_base.py:214 FileDeliveryStatus: Validated expected count for TEST_DIAL_V2
INFO     betl_src_poc_logger:export_parquet.py:56 Exported to sourcing_landing_data/DIAL/epr/TEST_DIAL_V2.parquet
INFO     betl_src_poc_logger:extract_base.py:214 FileDeliveryStatus: Copied parquet to storage account for TEST_DIAL_V2
INFO     betl_src_poc_logger:extract_base.py:214 FileDeliveryStatus: Moved JSON trigger file for TEST_DIAL_V2
============================================================================================ warnings summary ============================================================================================ 
..\bsrc-etl-venv\lib\site-packages\holidays\deprecations\v1_incompatibility.py:40
  C:\Users\B25712\bsrc-etl-venv\bsrc-etl-venv\lib\site-packages\holidays\deprecations\v1_incompatibility.py:40: FutureIncompatibilityWarning:

  This is a future version incompatibility warning from Holidays v0.62
  to inform you about an upcoming change in our API versioning strategy that may affect your
  project's dependencies. Starting from version 1.0 onwards, we will be following a loose form of
  Semantic Versioning (SemVer, https://semver.org) to provide clearer communication regarding any
  potential breaking changes.

  This means that while we strive to maintain backward compatibility, there might be occasional
  updates that introduce breaking changes to our API. To ensure the stability of your projects,
  we highly recommend pinning the version of our API that you rely on. You can pin your current
  holidays v0.x dependency (e.g., holidays==0.62) or limit it (e.g., holidays<1.0) in order to
  avoid potentially unwanted upgrade to the version 1.0 when it's released (ETA 2025Q1-Q2).

  If you have any questions or concerns regarding this change, please don't hesitate to reach out
  to us via https://github.com/vacanza/holidays/discussions/1800.

    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html

---------- coverage: platform win32, python 3.10.11-final-0 ----------
Name                                            Stmts   Miss  Cover   Missing
-----------------------------------------------------------------------------
src\__init__.py                                     0      0   100%
src\config\__init__.py                              0      0   100%
src\config\business_logic.py                       52      0   100%
src\config\constants.py                             1      0   100%
src\config\exceptions.py                           21     21     0%   6-48
src\config\process.py                               4      4     0%   7-10
src\config\schema.py                                4      4     0%   3-52
src\dq\__init__.py                                  0      0   100%
src\dq\dq_validation.py                           141    132     6%   47-112, 166-181, 197-249, 265-286, 318-350, 372-392, 414-486
src\extract\__init__.py                             0      0   100%
src\extract\master_data_sql.py                     90     71    21%   31-33, 37-38, 42, 59-88, 109-137, 149-156, 162-164, 188, 222-231, 285-300, 329-356
src\month_setup\__init__.py                         0      0   100%
src\month_setup\dial_derive_snapshotdate.py        32     27    16%   12-18, 28-37, 47-50, 69-85
src\month_setup\metadata_log_tables.py             32     25    22%   23-84, 159, 166-203
src\month_setup\setup_new_month.py                 13     13     0%   17-84
src\staging\__init__.py                             0      0   100%
src\staging\extract_base.py                        85     33    61%   165, 171-173, 320-321, 344-348, 368-404, 422-465
src\staging\extract_dial_data.py                   65      5    92%   330-347
src\staging\extract_nonssf_data.py                136    136     0%   16-532
src\staging\extract_ssf_data.py                   164    164     0%   26-620
src\staging\status.py                              57      5    91%   18, 53-54, 129, 151
src\transform\__init__.py                           0      0   100%
src\transform\table_write_and_comment.py           72     72     0%   14-237
src\transform\transform_business_logic_sql.py       6      6     0%   6-25
src\utils\__init__.py                               0      0   100%
src\utils\alias_util.py                            13     13     0%   10-109
src\utils\export_parquet.py                        15      6    60%   49-54, 67-68
src\utils\get_dbutils.py                            2      0   100%
src\utils\get_env.py                               10      0   100%
src\utils\logging_util.py                           6      0   100%
src\utils\parameter_utils.py                       23     19    17%   33-53, 79-84, 105-116
src\utils\parse_yaml.py                            22     22     0%   11-127
src\utils\sources_util.py                          52     52     0%   7-218
src\utils\table_logging.py                         14      3    79%   36-37, 55
src\utils\table_schema.py                           3      3     0%   8-16
src\utils\transformations_util.py                  17     12    29%   20-25, 39, 51, 65-68
src\validate\__init__.py                            0      0   100%
src\validate\base.py                                4      4     0%   4-7
src\validate\expressions.py                        27     27     0%   15-75
src\validate\run_all.py                             7      7     0%   12-48
src\validate\sources.py                            29     29     0%   7-67
src\validate\transformations.py                   192    192     0%   21-593
src\validate\validate_sql.py                       54     54     0%   13-130
src\validate\yaml.py                               18     18     0%   3-34
-----------------------------------------------------------------------------
TOTAL                                            1483   1179    20%
Coverage HTML written to dir htmlcov

======================================================================================== short test summary info ========================================================================================= 
FAILED test/staging/test_extract_dial_data.py::test_extract_dial_data[202503-test-container] - TypeError: ExtractStagingData.save_to_stg_table() got an unexpected keyword argument 'snapshot_date'        
================================================================================ 1 failed, 1 warning in 68.86s (0:01:08) ================================================================================= 
SUCCESS: The process with PID 26012 (child process of PID 26824) has been terminated.
SUCCESS: The process with PID 26824 (child process of PID 4172) has been terminated.
SUCCESS: The process with PID 4172 (child process of PID 10344) has been terminated.
