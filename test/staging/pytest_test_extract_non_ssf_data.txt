(bsrc-etl-venv) PS C:\Users\B25712\bsrc-etl-venv\bsrc-etl> pytest test/staging/test_extract_non_ssf_data.py
====================================================================================== test session starts =======================================================================================
platform win32 -- Python 3.10.11, pytest-8.3.3, pluggy-1.5.0
rootdir: C:\Users\B25712\bsrc-etl-venv\bsrc-etl
configfile: pyproject.toml
plugins: cov-6.0.0, mock-3.14.0
collected 4 items

test\staging\test_extract_non_ssf_data.py FFFF                                                                                                                                              [100%]

============================================================================================ FAILURES ============================================================================================ 
________________________________________________________________________ test_extract_non_ssf_data[202503-test-container] ________________________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001D2A94B3370>, mocker = <pytest_mock.plugin.MockerFixture object at 0x000001D2A94B0850>, run_month = '202503'
source_container = 'test-container', caplog = <_pytest.logging.LogCaptureFixture object at 0x000001D2A9486560>

    @pytest.mark.parametrize(
        ("run_month", "source_container"),
        [("202503", "test-container")],
    )
    def test_extract_non_ssf_data(
        spark_session,
        mocker,
        run_month,
        source_container,
        caplog,
    ):
        test_container = f"abfss://{source_container}@bsrcdadls.dfs.core.windows.net"
        month_container = f"abfss://{run_month}@bsrcdadls.dfs.core.windows.net"
        metadata_path = f"bsrc_d.metadata_{run_month}.metadata_nonssf"
        log_path = f"bsrc_d.log_{run_month}.log_nonssf"

        # Create a mock DataFrame
        schema_meta = [
            "SourceSystem",
            "SourceFileName",
            "SourceFileFormat",
            "SourceFileDelimiter",
            "StgTableName",
            "FileDeliveryStep",
            "FileDeliveryStatus",
        ]
        mock_meta = spark_session.createDataFrame(
            [
                (
                    "lrd_static",
                    "TEST_NON_SSF_V1",
                    ".txt",
                    "|",
                    "test_non_ssf_v1",
                    0,
                    "Expected",
                ),
                (
                    "lrd_static",
                    "TEST_NON_SSF_V2",
                    ".txt",
                    "|",
                    "test_non_ssf_v2",
                    0,
                    "Expected",
                ),
                (
                    "nme",
                    "TEST_NON_SSF_V3",
                    ".parquet",
                    ",",
                    "test_non_ssf_v3",
                    0,
                    "Expected",
                ),
                (
                    "finob",
                    "TEST_NON_SSF_V4",
                    ".csv",
                    ",",
                    "test_non_ssf_v4",
                    0,
                    "Expected",
                ),
            ],
            schema=schema_meta,
        )

        schema_log = [
            "SourceSystem",
            "SourceFileName",
            "DeliveryNumber",
            "FileDeliveryStep",
            "FileDeliveryStatus",
            "Result",
            "LastUpdatedDateTimestamp",
            "Comment",
        ]
        mock_log = spark_session.createDataFrame(
            [
                (
                    "lrd_static",
                    "TEST_NON_SSF_V1",
                    1,
                    0,
                    "Expected",
                    "Success",
                    datetime.now(timezone.utc),
                    "Test comment",
                )
            ],
            schema=schema_log,
        )

        dummy_df = spark_session.createDataFrame(
            [(1, "2", 3)],
            schema=StructType(
                [
                    StructField("col1", IntegerType()),
                    StructField("col2", StringType()),
                    StructField("col3", IntegerType()),
                ]
            ),
        )

        # Mock spark.read.json and spark.read.table to return the mock DataFrames
        mock_read = mocker.patch("pyspark.sql.SparkSession.read", autospec=True)
        mock_read.table.side_effect = [
            mock_meta,
            mock_log,
            dummy_df,
            dummy_df,
            dummy_df,
            dummy_df,
        ]

        mock_write = mocker.patch("pyspark.sql.DataFrameWriter.parquet")
        mock_save_table = mocker.patch("pyspark.sql.DataFrameWriter.saveAsTable")

        # Check ExtractNonSSFData class initialisation
        extraction = ExtractNonSSFData(
            spark_session,
            run_month,
            source_container=source_container,
        )

        # Verify that spark.read.table was called with the correct arguments
        mock_read.table.assert_any_call(f"bsrc_d.metadata_{run_month}.metadata_nonssf")
        mock_read.table.assert_any_call(f"bsrc_d.log_{run_month}.log_nonssf")

        mock_dbutils_fs_ls = mocker.patch.object(extraction.dbutils.fs, "ls")
        effect = [
            [
                FileInfoMock(
                    {
                        "path": f"{test_container}/{folder}/{file}",
                        "name": f"{file}",
                    }
                )
                for file, folder in li
            ]
            for li in [
                [
                    ("TEST_NON_SSF_V3.parquet", "NME"),
                    ("TEST_NON_SSF_V3.csv", "NME"),
                    ("processed/", "NME"),
                ],
                [("TEST_NON_SSF_V4.csv", "FINOB"), ("processed/", "FINOB")],
                [
                    ("TEST_NON_SSF_V1.txt", "LRD_STATIC"),
                    ("TEST_NON_SSF_V5.txt", "LRD_STATIC"),
                    ("processed/", "LRD_STATIC"),
                ],
                [("TEST_NON_SSF_V2_999999.txt", "LRD_STATIC/processed")],
                [("TEST_NON_SSF_V2_999999.txt", "LRD_STATIC/processed")],
            ]
        ]
        mock_dbutils_fs_ls.side_effect = effect
        mock_dbutils_fs_cp = mocker.patch.object(extraction.dbutils.fs, "cp")
        mock_dbutils_fs_mv = mocker.patch.object(extraction.dbutils.fs, "mv")

        # Test with deadline passed (default behavior in the test)
>       found_files = extraction.get_all_files(deadline_passed=True)

test\staging\test_extract_non_ssf_data.py:184:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  
src\staging\extract_nonssf_data.py:240: in get_all_files
    all_files["LRD_STATIC"] = self.place_static_data(
src\staging\extract_nonssf_data.py:155: in place_static_data
    if self.dbutils.fs.ls(latest_processed_file):
C:\Program Files\Python310\lib\unittest\mock.py:1114: in __call__
    return self._mock_call(*args, **kwargs)
C:\Program Files\Python310\lib\unittest\mock.py:1118: in _mock_call
    return self._execute_mock_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  

self = <MagicMock name='ls' id='2004296482384'>, args = ('abfss://test-container@bsrcdadls.dfs.core.windows.net/LRD_STATIC/processed/TEST_NON_SSF_V2_999999.txt',), kwargs = {}
effect = <list_iterator object at 0x000001D2A96301F0>

    def _execute_mock_call(self, /, *args, **kwargs):
        # separate from _increment_mock_call so that awaited functions are
        # executed separately from their call, also AsyncMock overrides this method

        effect = self.side_effect
        if effect is not None:
            if _is_exception(effect):
                raise effect
            elif not _callable(effect):
>               result = next(effect)
E               StopIteration

C:\Program Files\Python310\lib\unittest\mock.py:1175: StopIteration

The above exception was the direct cause of the following exception:

cls = <class '_pytest.runner.CallInfo'>, func = <function call_and_report.<locals>.<lambda> at 0x000001D2A94D13F0>, when = 'call'
reraise = (<class '_pytest.outcomes.Exit'>, <class 'KeyboardInterrupt'>)

    @classmethod
    def from_call(
        cls,
        func: Callable[[], TResult],
        when: Literal["collect", "setup", "call", "teardown"],
        reraise: type[BaseException] | tuple[type[BaseException], ...] | None = None,
    ) -> CallInfo[TResult]:
        """Call func, wrapping the result in a CallInfo.

        :param func:
            The function to call. Called without arguments.
        :type func: Callable[[], _pytest.runner.TResult]
        :param when:
            The phase in which the function is called.
        :param reraise:
            Exception or exceptions that shall propagate if raised by the
            function, instead of being wrapped in the CallInfo.
        """
        excinfo = None
        start = timing.time()
        precise_start = timing.perf_counter()
        try:
>           result: TResult | None = func()

..\bsrc-etl-venv\lib\site-packages\_pytest\runner.py:341:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  
..\bsrc-etl-venv\lib\site-packages\_pytest\runner.py:242: in <lambda>
    lambda: runtest_hook(item=item, **kwds), when=when, reraise=reraise
..\bsrc-etl-venv\lib\site-packages\pluggy\_hooks.py:513: in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
..\bsrc-etl-venv\lib\site-packages\pluggy\_manager.py:120: in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
..\bsrc-etl-venv\lib\site-packages\_pytest\threadexception.py:92: in pytest_runtest_call
    yield from thread_exception_runtest_hook()
..\bsrc-etl-venv\lib\site-packages\_pytest\threadexception.py:68: in thread_exception_runtest_hook
    yield
..\bsrc-etl-venv\lib\site-packages\_pytest\unraisableexception.py:95: in pytest_runtest_call
    yield from unraisable_exception_runtest_hook()
..\bsrc-etl-venv\lib\site-packages\_pytest\unraisableexception.py:70: in unraisable_exception_runtest_hook
    yield
..\bsrc-etl-venv\lib\site-packages\_pytest\logging.py:846: in pytest_runtest_call
    yield from self._runtest_for(item, "call")
..\bsrc-etl-venv\lib\site-packages\_pytest\logging.py:829: in _runtest_for
    yield
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  

self = <CaptureManager _method='fd' _global_capturing=<MultiCapture out=<FDCapture 1 oldfd=9 _state='suspended' tmpfile=<_io...._io.TextIOWrapper name='nul' mode='r' encoding='utf-8'>> _state='suspended' _in_suspended=False> _capture_fixture=None>
item = <Function test_extract_non_ssf_data[202503-test-container]>

    @hookimpl(wrapper=True)
    def pytest_runtest_call(self, item: Item) -> Generator[None]:
        with self.item_capture("call", item):
>           return (yield)
E           RuntimeError: generator raised StopIteration

..\bsrc-etl-venv\lib\site-packages\_pytest\capture.py:880: RuntimeError
------------------------------------------------------------------------------------- Captured stderr setup -------------------------------------------------------------------------------------- 
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
-------------------------------------------------------------------------------------- Captured stdout call -------------------------------------------------------------------------------------- 
2025-06-18 11:15:13 [WARNING] get_all_files:  File TEST_NON_SSF_V3 not found in metadata. Please check if it should be delivered.
2025-06-18 11:15:13 [WARNING] get_all_files:  File TEST_NON_SSF_V3 not found in metadata. Please check if it should be delivered.
2025-06-18 11:15:13 [WARNING] get_all_files:  File TEST_NON_SSF_V3 not found in metadata. Please check if it should be delivered.
2025-06-18 11:15:13 [WARNING] get_all_files:  File TEST_NON_SSF_V3 not found in metadata. Please check if it should be delivered.
2025-06-18 11:15:15 [WARNING] get_all_files:  File TEST_NON_SSF_V4 not found in metadata. Please check if it should be delivered.
2025-06-18 11:15:15 [WARNING] get_all_files:  File TEST_NON_SSF_V4 not found in metadata. Please check if it should be delivered.
2025-06-18 11:15:16 [WARNING] get_all_files:  File TEST_NON_SSF_V1 not found in metadata. Please check if it should be delivered.
2025-06-18 11:15:16 [WARNING] get_all_files:  File TEST_NON_SSF_V1 not found in metadata. Please check if it should be delivered.
2025-06-18 11:15:16 [WARNING] get_all_files:  File TEST_NON_SSF_V5 not found in metadata. Please check if it should be delivered.
2025-06-18 11:15:16 [WARNING] get_all_files:  File TEST_NON_SSF_V5 not found in metadata. Please check if it should be delivered.
2025-06-18 11:15:21 [ERROR] place_static_data:  File TEST_NON_SSF_V1 not delivered and not found in LRD_STATIC/processed folder.
2025-06-18 11:15:21 [ERROR] place_static_data:  File TEST_NON_SSF_V1 not delivered and not found in LRD_STATIC/processed folder.
-------------------------------------------------------------------------------------- Captured stderr call -------------------------------------------------------------------------------------- 

--------------------------------------------------------------------------------------- Captured log call ---------------------------------------------------------------------------------------- 
WARNING  betl_src_poc_logger:extract_nonssf_data.py:227 File TEST_NON_SSF_V3 not found in metadata. Please check if it should be delivered.
WARNING  betl_src_poc_logger:extract_nonssf_data.py:227 File TEST_NON_SSF_V3 not found in metadata. Please check if it should be delivered.
WARNING  betl_src_poc_logger:extract_nonssf_data.py:227 File TEST_NON_SSF_V4 not found in metadata. Please check if it should be delivered.
WARNING  betl_src_poc_logger:extract_nonssf_data.py:227 File TEST_NON_SSF_V1 not found in metadata. Please check if it should be delivered.
WARNING  betl_src_poc_logger:extract_nonssf_data.py:227 File TEST_NON_SSF_V5 not found in metadata. Please check if it should be delivered.
ERROR    betl_src_poc_logger:extract_nonssf_data.py:148 File TEST_NON_SSF_V1 not delivered and not found in LRD_STATIC/processed folder.
______________________________________________________________ test_extract_non_ssf_data_with_deadline[202503-test-container-True] _______________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001D2A94B3370>, mocker = <pytest_mock.plugin.MockerFixture object at 0x000001D2A94B01C0>, run_month = '202503'
source_container = 'test-container', deadline_passed = True, caplog = <_pytest.logging.LogCaptureFixture object at 0x000001D2A6C64460>

    @pytest.mark.parametrize(
        ("run_month", "source_container", "deadline_passed"),
        [
            ("202503", "test-container", True),
            ("202503", "test-container", False),
        ],
    )
    def test_extract_non_ssf_data_with_deadline(
        spark_session,
        mocker,
        run_month,
        source_container,
        deadline_passed,
        caplog,
    ):
        """Test the deadline functionality for LRD_STATIC files."""
        test_container = f"abfss://{source_container}@bsrcdadls.dfs.core.windows.net"

        # Create mock metadata DataFrame
        schema_meta = [
            "SourceSystem",
            "SourceFileName",
            "SourceFileFormat",
            "SourceFileDelimiter",
            "StgTableName",
            "FileDeliveryStep",
            "FileDeliveryStatus",
        ]
        mock_meta = spark_session.createDataFrame(
            [
                (
                    "lrd_static",
                    "TEST_STATIC_FILE",
                    ".txt",
                    "|",
                    "test_static_file",
                    0,
                    "Expected",
                ),
            ],
            schema=schema_meta,
        )

        schema_log = [
            "SourceSystem",
            "SourceFileName",
            "DeliveryNumber",
            "FileDeliveryStep",
            "FileDeliveryStatus",
            "Result",
            "LastUpdatedDateTimestamp",
            "Comment",
        ]
>       mock_log = spark_session.createDataFrame(
            [],
            schema=schema_log,  # Empty log
        )

test\staging\test_extract_non_ssf_data.py:329:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  
..\bsrc-etl-venv\lib\site-packages\pyspark\sql\session.py:1443: in createDataFrame
    return self._create_dataframe(
..\bsrc-etl-venv\lib\site-packages\pyspark\sql\session.py:1485: in _create_dataframe
    rdd, struct = self._createFromLocal(map(prepare, data), schema)
..\bsrc-etl-venv\lib\site-packages\pyspark\sql\session.py:1093: in _createFromLocal
    struct = self._inferSchemaFromList(data, names=schema)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  

self = <pyspark.sql.session.SparkSession object at 0x000001D2A94B3370>, data = []
names = ['SourceSystem', 'SourceFileName', 'DeliveryNumber', 'FileDeliveryStep', 'FileDeliveryStatus', 'Result', ...]

    def _inferSchemaFromList(
        self, data: Iterable[Any], names: Optional[List[str]] = None
    ) -> StructType:
        """
        Infer schema from list of Row, dict, or tuple.

        Parameters
        ----------
        data : iterable
            list of Row, dict, or tuple
        names : list, optional
            list of column names

        Returns
        -------
        :class:`pyspark.sql.types.StructType`
        """
        if not data:
>           raise PySparkValueError(
                error_class="CANNOT_INFER_EMPTY_SCHEMA",
                message_parameters={},
            )
E           pyspark.errors.exceptions.base.PySparkValueError: [CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from empty dataset.

..\bsrc-etl-venv\lib\site-packages\pyspark\sql\session.py:948: PySparkValueError
______________________________________________________________ test_extract_non_ssf_data_with_deadline[202503-test-container-False] ______________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001D2A94B3370>, mocker = <pytest_mock.plugin.MockerFixture object at 0x000001D2A9CD3F70>, run_month = '202503'
source_container = 'test-container', deadline_passed = False, caplog = <_pytest.logging.LogCaptureFixture object at 0x000001D2A9CD33D0>

    @pytest.mark.parametrize(
        ("run_month", "source_container", "deadline_passed"),
        [
            ("202503", "test-container", True),
            ("202503", "test-container", False),
        ],
    )
    def test_extract_non_ssf_data_with_deadline(
        spark_session,
        mocker,
        run_month,
        source_container,
        deadline_passed,
        caplog,
    ):
        """Test the deadline functionality for LRD_STATIC files."""
        test_container = f"abfss://{source_container}@bsrcdadls.dfs.core.windows.net"
    
        # Create mock metadata DataFrame
        schema_meta = [
            "SourceSystem",
            "SourceFileName",
            "SourceFileFormat",
            "SourceFileDelimiter",
            "StgTableName",
            "FileDeliveryStep",
            "FileDeliveryStatus",
        ]
        mock_meta = spark_session.createDataFrame(
            [
                (
                    "lrd_static",
                    "TEST_STATIC_FILE",
                    ".txt",
                    "|",
                    "test_static_file",
                    0,
                    "Expected",
                ),
            ],
            schema=schema_meta,
        )

        schema_log = [
            "SourceSystem",
            "SourceFileName",
            "DeliveryNumber",
            "FileDeliveryStep",
            "FileDeliveryStatus",
            "Result",
            "LastUpdatedDateTimestamp",
            "Comment",
        ]
>       mock_log = spark_session.createDataFrame(
            [],
            schema=schema_log,  # Empty log
        )

test\staging\test_extract_non_ssf_data.py:329:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  
..\bsrc-etl-venv\lib\site-packages\pyspark\sql\session.py:1443: in createDataFrame
    return self._create_dataframe(
..\bsrc-etl-venv\lib\site-packages\pyspark\sql\session.py:1485: in _create_dataframe
    rdd, struct = self._createFromLocal(map(prepare, data), schema)
..\bsrc-etl-venv\lib\site-packages\pyspark\sql\session.py:1093: in _createFromLocal
    struct = self._inferSchemaFromList(data, names=schema)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  

self = <pyspark.sql.session.SparkSession object at 0x000001D2A94B3370>, data = []
names = ['SourceSystem', 'SourceFileName', 'DeliveryNumber', 'FileDeliveryStep', 'FileDeliveryStatus', 'Result', ...]

    def _inferSchemaFromList(
        self, data: Iterable[Any], names: Optional[List[str]] = None
    ) -> StructType:
        """
        Infer schema from list of Row, dict, or tuple.

        Parameters
        ----------
        data : iterable
            list of Row, dict, or tuple
        names : list, optional
            list of column names

        Returns
        -------
        :class:`pyspark.sql.types.StructType`
        """
        if not data:
>           raise PySparkValueError(
                error_class="CANNOT_INFER_EMPTY_SCHEMA",
                message_parameters={},
            )
E           pyspark.errors.exceptions.base.PySparkValueError: [CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from empty dataset.

..\bsrc-etl-venv\lib\site-packages\pyspark\sql\session.py:948: PySparkValueError
___________________________________________________________________ test_place_static_data_keyword_only[202503-test-container] ___________________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x000001D2A94B3370>, mocker = <pytest_mock.plugin.MockerFixture object at 0x000001D2A98BB7C0>, run_month = '202503'
source_container = 'test-container'

    @pytest.mark.parametrize(
        ("run_month", "source_container"),
        [("202503", "test-container")],
    )
    def test_place_static_data_keyword_only(
        spark_session,
        mocker,
        run_month,
        source_container,
    ):
        """Test that place_static_data requires deadline_passed as keyword argument."""
        # Create mock metadata DataFrame
        schema_meta = [
            "SourceSystem",
            "SourceFileName",
            "SourceFileFormat",
            "SourceFileDelimiter",
            "StgTableName",
            "FileDeliveryStep",
            "FileDeliveryStatus",
        ]
        mock_meta = spark_session.createDataFrame(
            [
                (
                    "lrd_static",
                    "TEST_FILE",
                    ".txt",
                    "|",
                    "test_file",
                    0,
                    "Expected",
                ),
            ],
            schema=schema_meta,
        )

        schema_log = [
            "SourceSystem",
            "SourceFileName",
            "DeliveryNumber",
            "FileDeliveryStep",
            "FileDeliveryStatus",
            "Result",
            "LastUpdatedDateTimestamp",
            "Comment",
        ]
>       mock_log = spark_session.createDataFrame([], schema=schema_log)

test\staging\test_extract_non_ssf_data.py:461:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  
..\bsrc-etl-venv\lib\site-packages\pyspark\sql\session.py:1443: in createDataFrame
    return self._create_dataframe(
..\bsrc-etl-venv\lib\site-packages\pyspark\sql\session.py:1485: in _create_dataframe
    rdd, struct = self._createFromLocal(map(prepare, data), schema)
..\bsrc-etl-venv\lib\site-packages\pyspark\sql\session.py:1093: in _createFromLocal
    struct = self._inferSchemaFromList(data, names=schema)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  

self = <pyspark.sql.session.SparkSession object at 0x000001D2A94B3370>, data = []
names = ['SourceSystem', 'SourceFileName', 'DeliveryNumber', 'FileDeliveryStep', 'FileDeliveryStatus', 'Result', ...]

    def _inferSchemaFromList(
        self, data: Iterable[Any], names: Optional[List[str]] = None
    ) -> StructType:
        """
        Infer schema from list of Row, dict, or tuple.

        Parameters
        ----------
        data : iterable
            list of Row, dict, or tuple
        names : list, optional
            list of column names

        Returns
        -------
        :class:`pyspark.sql.types.StructType`
        """
        if not data:
>           raise PySparkValueError(
                error_class="CANNOT_INFER_EMPTY_SCHEMA",
                message_parameters={},
            )
E           pyspark.errors.exceptions.base.PySparkValueError: [CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from empty dataset.

..\bsrc-etl-venv\lib\site-packages\pyspark\sql\session.py:948: PySparkValueError
======================================================================================== warnings summary ======================================================================================== 
test\staging\test_extract_non_ssf_data.py:235
test\staging\test_extract_non_ssf_data.py:235
  C:\Users\B25712\bsrc-etl-venv\bsrc-etl\test\staging\test_extract_non_ssf_data.py:235: DeprecationWarning: invalid escape sequence '\d'
    f"{test_container}/{source_system}/processed/{file_name_no_ext}__\d{{8}}\d{{6}}{file_name_ext}",  # noqa: W605

..\bsrc-etl-venv\lib\site-packages\holidays\deprecations\v1_incompatibility.py:40
  C:\Users\B25712\bsrc-etl-venv\bsrc-etl-venv\lib\site-packages\holidays\deprecations\v1_incompatibility.py:40: FutureIncompatibilityWarning:

  This is a future version incompatibility warning from Holidays v0.62
  to inform you about an upcoming change in our API versioning strategy that may affect your
  project's dependencies. Starting from version 1.0 onwards, we will be following a loose form of
  Semantic Versioning (SemVer, https://semver.org) to provide clearer communication regarding any
  potential breaking changes.

  This means that while we strive to maintain backward compatibility, there might be occasional
  updates that introduce breaking changes to our API. To ensure the stability of your projects,
  we highly recommend pinning the version of our API that you rely on. You can pin your current
  holidays v0.x dependency (e.g., holidays==0.62) or limit it (e.g., holidays<1.0) in order to
  avoid potentially unwanted upgrade to the version 1.0 when it's released (ETA 2025Q1-Q2).

  If you have any questions or concerns regarding this change, please don't hesitate to reach out
  to us via https://github.com/vacanza/holidays/discussions/1800.

    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html

---------- coverage: platform win32, python 3.10.11-final-0 ----------
Name                                            Stmts   Miss  Cover   Missing
-----------------------------------------------------------------------------
src\__init__.py                                     0      0   100%
src\config\__init__.py                              0      0   100%
src\config\business_logic.py                       52      0   100%
src\config\constants.py                             1      0   100%
src\config\exceptions.py                           21     21     0%   6-48
src\config\process.py                               4      4     0%   7-10
src\config\schema.py                                4      4     0%   3-52
src\dq\__init__.py                                  0      0   100%
src\dq\dq_validation.py                           141    132     6%   47-112, 166-181, 197-249, 265-286, 318-350, 372-392, 414-486
src\extract\__init__.py                             0      0   100%
src\extract\master_data_sql.py                     90     71    21%   31-33, 37-38, 42, 59-88, 109-137, 149-156, 162-164, 188, 222-231, 285-300, 329-356
src\month_setup\__init__.py                         0      0   100%
src\month_setup\dial_derive_snapshotdate.py        32     27    16%   12-18, 28-37, 47-50, 69-85
src\month_setup\metadata_log_tables.py             32     28    12%   23-84, 88-159, 166-203
src\month_setup\setup_new_month.py                 13     13     0%   17-84
src\staging\__init__.py                             0      0   100%
src\staging\extract_base.py                        65     38    42%   107-113, 155-176, 205-219, 275-321, 343-352, 386-403, 419
src\staging\extract_dial_data.py                   65     65     0%   16-360
src\staging\extract_nonssf_data.py                103     58    44%   48-70, 111-115, 128-132, 141-145, 157-182, 211-213, 233, 245-258, 278-312, 327, 343, 366-388
src\staging\extract_ssf_data.py                   164    164     0%   26-620
src\staging\status.py                              57      9    84%   18, 26, 35, 53-54, 63, 107, 129, 151
src\transform\__init__.py                           0      0   100%
src\transform\table_write_and_comment.py           72     72     0%   14-237
src\transform\transform_business_logic_sql.py       6      6     0%   6-25
src\utils\__init__.py                               0      0   100%
src\utils\alias_util.py                            13     13     0%   10-109
src\utils\export_parquet.py                        15     12    20%   38-57, 67-68
src\utils\get_dbutils.py                            2      0   100%
src\utils\get_env.py                               10      0   100%
src\utils\logging_util.py                           6      0   100%
src\utils\parameter_utils.py                       23     19    17%   33-53, 79-84, 105-116
src\utils\parse_yaml.py                            22     22     0%   11-127
src\utils\sources_util.py                          52     52     0%   7-218
src\utils\table_logging.py                         14     11    21%   25-37, 54-56
src\utils\table_schema.py                           3      3     0%   8-16
src\utils\transformations_util.py                  17     12    29%   20-25, 39, 51, 65-68
src\validate\__init__.py                            0      0   100%
src\validate\base.py                                4      4     0%   4-7
src\validate\expressions.py                        27     27     0%   15-75
src\validate\run_all.py                             7      7     0%   12-48
src\validate\sources.py                            29     29     0%   7-67
src\validate\transformations.py                   192    192     0%   21-593
src\validate\validate_sql.py                       54     54     0%   13-130
src\validate\yaml.py                               18     18     0%   3-34
-----------------------------------------------------------------------------
TOTAL                                            1430   1187    17%
Coverage HTML written to dir htmlcov

==================================================================================== short test summary info ===================================================================================== 
FAILED test/staging/test_extract_non_ssf_data.py::test_extract_non_ssf_data[202503-test-container] - RuntimeError: generator raised StopIteration
FAILED test/staging/test_extract_non_ssf_data.py::test_extract_non_ssf_data_with_deadline[202503-test-container-True] - pyspark.errors.exceptions.base.PySparkValueError: [CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from empty dataset.
FAILED test/staging/test_extract_non_ssf_data.py::test_extract_non_ssf_data_with_deadline[202503-test-container-False] - pyspark.errors.exceptions.base.PySparkValueError: [CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from empty dataset.
FAILED test/staging/test_extract_non_ssf_data.py::test_place_static_data_keyword_only[202503-test-container] - pyspark.errors.exceptions.base.PySparkValueError: [CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from empty dataset.
================================================================================= 4 failed, 3 warnings in 37.82s ================================================================================= 
SUCCESS: The process with PID 11824 (child process of PID 19808) has been terminated.
SUCCESS: The process with PID 19808 (child process of PID 25292) has been terminated.
SUCCESS: The process with PID 25292 (child process of PID 26284) has been terminated.