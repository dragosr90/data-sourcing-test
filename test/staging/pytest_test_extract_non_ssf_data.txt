(bsrc-etl-venv) PS C:\Users\B25712\bsrc-etl-venv\bsrc-etl> pytest test/staging/test_extract_non_ssf_data.py
========================================================================================== test session starts ===========================================================================================
platform win32 -- Python 3.10.11, pytest-8.3.3, pluggy-1.5.0
rootdir: C:\Users\B25712\bsrc-etl-venv\bsrc-etl
configfile: pyproject.toml
plugins: cov-6.0.0, mock-3.14.0
collected 3 items

test\staging\test_extract_non_ssf_data.py F.F                                                                                                                                                       [100%]

================================================================================================ FAILURES ================================================================================================ 
____________________________________________________________________________ test_extract_non_ssf_data[202505-test-container] ____________________________________________________________________________ 

spark_session = <pyspark.sql.session.SparkSession object at 0x0000016D1F61E590>, mocker = <pytest_mock.plugin.MockerFixture object at 0x0000016D1F61DD50>, run_month = '202505'
source_container = 'test-container', caplog = <_pytest.logging.LogCaptureFixture object at 0x0000016D1F61C340>

    @pytest.mark.parametrize(
        ("run_month", "source_container"),
        [("202505", "test-container")],
    )
    def test_extract_non_ssf_data(
        spark_session,
        mocker,
        run_month,
        source_container,
        caplog,
    ):
        test_container = f"abfss://{source_container}@bsrcdadls.dfs.core.windows.net"
        month_container = f"abfss://{run_month}@bsrcdadls.dfs.core.windows.net"
        metadata_path = f"bsrc_d.metadata_{run_month}.metadata_nonssf"
        log_path = f"bsrc_d.log_{run_month}.log_nonssf"

        # Create deadline dates
        future_date = (datetime.now(timezone.utc) + timedelta(days=10)).strftime("%Y-%m-%d")
        past_date = (datetime.now(timezone.utc) - timedelta(days=5)).strftime("%Y-%m-%d")

        # Create a mock DataFrame with deadline column
        schema_meta = [
            "SourceSystem",
            "SourceFileName",
            "SourceFileFormat",
            "SourceFileDelimiter",
            "StgTableName",
            "FileDeliveryStep",
            "FileDeliveryStatus",
            "Deadline",
        ]
        mock_meta = spark_session.createDataFrame(
            [
                (
                    "lrd_static",
                    "TEST_NON_SSF_V1",
                    ".txt",
                    "|",
                    "test_non_ssf_v1",
                    0,
                    "Expected",
                    future_date,  # Future deadline
                ),
                (
                    "lrd_static",
                    "TEST_NON_SSF_V2",
                    ".txt",
                    "|",
                    "test_non_ssf_v2",
                    0,
                    "Expected",
                    past_date,  # Past deadline - should be copied
                ),
                (
                    "nme",
                    "TEST_NON_SSF_V3",
                    ".parquet",
                    ",",
                    "test_non_ssf_v3",
                    0,
                    "Expected",
                    future_date,
                ),
                (
                    "finob",
                    "TEST_NON_SSF_V4",
                    ".csv",
                    ",",
                    "test_non_ssf_v4",
                    0,
                    "Expected",
                    past_date,  # Past deadline
                ),
            ],
            schema=schema_meta,
        )

        schema_log = [
            "SourceSystem",
            "SourceFileName",
            "DeliveryNumber",
            "FileDeliveryStep",
            "FileDeliveryStatus",
            "Result",
            "LastUpdatedDateTimestamp",
            "Comment",
        ]
        mock_log = spark_session.createDataFrame(
            [
                (
                    "lrd_static",
                    "TEST_NON_SSF_V1",
                    1,
                    0,
                    "Expected",
                    "Success",
                    datetime.now(timezone.utc),
                    "Test comment",
                )
            ],
            schema=schema_log,
        )

        dummy_df = spark_session.createDataFrame(
            [(1, "2", 3)],
            schema=StructType(
                [
                    StructField("col1", IntegerType()),
                    StructField("col2", StringType()),
                    StructField("col3", IntegerType()),
                ]
            ),
        )

        # Mock spark.read.json and spark.read.table to return the mock DataFrames
        mock_read = mocker.patch("pyspark.sql.SparkSession.read", autospec=True)
        mock_read.table.side_effect = [
            mock_meta,
            mock_log,
            dummy_df,
            dummy_df,
            dummy_df,
            dummy_df,
        ]

        mock_write = mocker.patch("pyspark.sql.DataFrameWriter.parquet")
        mock_save_table = mocker.patch("pyspark.sql.DataFrameWriter.saveAsTable")

        # Check ExtractNonSSFData class initialisation
        extraction = ExtractNonSSFData(
            spark_session,
            run_month,
            source_container=source_container,
        )

        # Initialize process log
        extraction.initialize_process_log(run_id=1)
        assert extraction.base_process_record["RunID"] == 1
        assert extraction.base_process_record["Component"] == "Non-SSF"

        # Verify that spark.read.table was called with the correct arguments
        mock_read.table.assert_any_call(f"bsrc_d.metadata_{run_month}.metadata_nonssf")
        mock_read.table.assert_any_call(f"bsrc_d.log_{run_month}.log_nonssf")

        # Test deadline checking
        deadline_reached, deadline_str = extraction.check_deadline_reached(
            "TEST_NON_SSF_V1"
        )
        assert not deadline_reached  # Future deadline
        assert deadline_str == future_date

        deadline_reached, deadline_str = extraction.check_deadline_reached(
            "TEST_NON_SSF_V2"
        )
        assert deadline_reached  # Past deadline
        assert deadline_str == past_date

        mock_dbutils_fs_ls = mocker.patch.object(extraction.dbutils.fs, "ls")
        effect = [
            [
                FileInfoMock(
                    {
                        "path": f"{test_container}/{folder}/{file}",
                        "name": f"{file}",
                    }
                )
                for file, folder in li
            ]
            for li in [
                [
                    ("TEST_NON_SSF_V3.parquet", "NME"),
                    ("TEST_NON_SSF_V3.csv", "NME"),
                    ("processed/", "NME"),
                ],
                [("TEST_NON_SSF_V4.csv", "FINOB"), ("processed/", "FINOB")],
                [
                    ("TEST_NON_SSF_V1.txt", "LRD_STATIC"),
                    ("TEST_NON_SSF_V5.txt", "LRD_STATIC"),
                    ("processed/", "LRD_STATIC"),
                ],
                [("TEST_NON_SSF_V2_999999.txt", "LRD_STATIC/processed")],
                [("TEST_NON_SSF_V2_999999.txt", "LRD_STATIC/processed")],
            ]
        ]
        mock_dbutils_fs_ls.side_effect = effect
        mock_dbutils_fs_cp = mocker.patch.object(extraction.dbutils.fs, "cp")
        mock_dbutils_fs_mv = mocker.patch.object(extraction.dbutils.fs, "mv")

        found_files = extraction.get_all_files()

        # V2 should be copied because deadline is reached
        mock_dbutils_fs_cp.assert_any_call(
            f"{test_container}/LRD_STATIC/processed/TEST_NON_SSF_V2_999999.txt",
            f"{test_container}/LRD_STATIC/TEST_NON_SSF_V2.txt",
        )

        # V1 should not be copied because deadline is not reached
>       assert "Deadline not reached for TEST_NON_SSF_V1" in caplog.text
E       AssertionError: assert 'Deadline not reached for TEST_NON_SSF_V1' in 'INFO     betl_src_poc_logger:extract_base.py:205 FileDeliveryStatus: Received / Placed for TEST_NON_SSF_V3\nINFO     ...reached.\nINFO     betl_src_poc_logger:extract_base.py:205 FileDeliveryStatus: Received / Placed for TEST_NON_SSF_V2\n'
E        +  where 'INFO     betl_src_poc_logger:extract_base.py:205 FileDeliveryStatus: Received / Placed for TEST_NON_SSF_V3\nINFO     ...reached.\nINFO     betl_src_poc_logger:extract_base.py:205 FileDeliveryStatus: Received / Placed for TEST_NON_SSF_V2\n' = <_pytest.logging.LogCaptureFixture object at 0x0000016D1F61C340>.text

test\staging\test_extract_non_ssf_data.py:219: AssertionError
----------------------------------------------------------------------------------------- Captured stderr setup ------------------------------------------------------------------------------------------ 
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
------------------------------------------------------------------------------------------ Captured stdout call ------------------------------------------------------------------------------------------ 
2025-07-17 15:10:10 [INFO] update_log_metadata:  FileDeliveryStatus: Received / Placed for TEST_NON_SSF_V3
2025-07-17 15:10:10 [INFO] update_log_metadata:  FileDeliveryStatus: Received / Placed for TEST_NON_SSF_V3
2025-07-17 15:10:13 [INFO] update_log_metadata:  FileDeliveryStatus: Received / Placed for TEST_NON_SSF_V3
2025-07-17 15:10:13 [INFO] update_log_metadata:  FileDeliveryStatus: Received / Placed for TEST_NON_SSF_V3
2025-07-17 15:10:16 [INFO] update_log_metadata:  FileDeliveryStatus: Received / Placed for TEST_NON_SSF_V4
2025-07-17 15:10:16 [INFO] update_log_metadata:  FileDeliveryStatus: Received / Placed for TEST_NON_SSF_V4
2025-07-17 15:10:19 [INFO] update_log_metadata:  FileDeliveryStatus: Received / Placed for TEST_NON_SSF_V1
2025-07-17 15:10:19 [INFO] update_log_metadata:  FileDeliveryStatus: Received / Placed for TEST_NON_SSF_V1
2025-07-17 15:10:20 [WARNING] get_all_files:  File TEST_NON_SSF_V5 not found in metadata. Please check if it should be delivered.
2025-07-17 15:10:20 [WARNING] get_all_files:  File TEST_NON_SSF_V5 not found in metadata. Please check if it should be delivered.
2025-07-17 15:10:23 [INFO] place_static_data:  Copied TEST_NON_SSF_V2 to static folder after deadline reached.
2025-07-17 15:10:23 [INFO] place_static_data:  Copied TEST_NON_SSF_V2 to static folder after deadline reached.
2025-07-17 15:10:23 [INFO] update_log_metadata:  FileDeliveryStatus: Received / Placed for TEST_NON_SSF_V2
2025-07-17 15:10:23 [INFO] update_log_metadata:  FileDeliveryStatus: Received / Placed for TEST_NON_SSF_V2
------------------------------------------------------------------------------------------ Captured stderr call ------------------------------------------------------------------------------------------ 

------------------------------------------------------------------------------------------- Captured log call -------------------------------------------------------------------------------------------- 
INFO     betl_src_poc_logger:extract_base.py:205 FileDeliveryStatus: Received / Placed for TEST_NON_SSF_V3
INFO     betl_src_poc_logger:extract_base.py:205 FileDeliveryStatus: Received / Placed for TEST_NON_SSF_V3
INFO     betl_src_poc_logger:extract_base.py:205 FileDeliveryStatus: Received / Placed for TEST_NON_SSF_V4
INFO     betl_src_poc_logger:extract_base.py:205 FileDeliveryStatus: Received / Placed for TEST_NON_SSF_V1
WARNING  betl_src_poc_logger:extract_nonssf_data.py:382 File TEST_NON_SSF_V5 not found in metadata. Please check if it should be delivered.
INFO     betl_src_poc_logger:extract_nonssf_data.py:239 Copied TEST_NON_SSF_V2 to static folder after deadline reached.
INFO     betl_src_poc_logger:extract_base.py:205 FileDeliveryStatus: Received / Placed for TEST_NON_SSF_V2
___________________________________________________________________________ test_append_to_process_log[202505-test-container] ____________________________________________________________________________ 

self = <MagicMock name='write_to_log' id='1568192947024'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'write_to_log' to have been called once. Called 0 times.

C:\Program Files\Python310\lib\unittest\mock.py:908: AssertionError

During handling of the above exception, another exception occurred:

spark_session = <pyspark.sql.session.SparkSession object at 0x0000016D1F61E590>, mocker = <pytest_mock.plugin.MockerFixture object at 0x0000016D1F836500>, run_month = '202505'
source_container = 'test-container'

    @pytest.mark.parametrize(
        ("run_month", "source_container"),
        [("202505", "test-container")],
    )
    def test_append_to_process_log(
        spark_session,
        mocker,
        run_month,
        source_container,
    ):
        """Test the append_to_process_log method."""
        # Mock metadata and log
        mock_meta = spark_session.createDataFrame([("dummy", 1)], schema=["col1", "col2"])
        mock_log = spark_session.createDataFrame([("dummy", 1)], schema=["col1", "col2"])

        # Mock spark read
        mock_read = mocker.patch("pyspark.sql.SparkSession.read", autospec=True)
        mock_read.table.side_effect = [mock_meta, mock_log]

        # Mock write_to_log
        mock_write_to_log = mocker.patch(
            "abnamro_bsrc_etl.utils.table_logging.write_to_log"
        )

        # Create extraction instance
        extraction = ExtractNonSSFData(
            spark_session, run_month, source_container=source_container
        )
        extraction.initialize_process_log(run_id=123)

        # Test append_to_process_log
        extraction.append_to_process_log(
            source_system="TEST_SYSTEM", comments="Test comment", status="Started"
        )

        # Verify write_to_log was called
>       mock_write_to_log.assert_called_once()
E       AssertionError: Expected 'write_to_log' to have been called once. Called 0 times.

test\staging\test_extract_non_ssf_data.py:527: AssertionError
------------------------------------------------------------------------------------------ Captured stdout call ------------------------------------------------------------------------------------------ 
2025-07-17 15:10:34 [ERROR] write_to_log:  Error writing to process log table log_202505.process_log
Traceback (most recent call last):
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl\src\abnamro_bsrc_etl\utils\table_logging.py", line 33, in write_to_log
    log_entry_df.write.mode("append").saveAsTable(
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl-venv\lib\site-packages\pyspark\sql\readwriter.py", line 1586, in saveAsTable
    self._jwrite.saveAsTable(name)
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl-venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl-venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Couldn't find a catalog to handle the identifier bsrc_d.log_202505.process_log.
2025-07-17 15:10:34 [ERROR] write_to_log:  Error writing to process log table log_202505.process_log
Traceback (most recent call last):
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl\src\abnamro_bsrc_etl\utils\table_logging.py", line 33, in write_to_log
    log_entry_df.write.mode("append").saveAsTable(
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl-venv\lib\site-packages\pyspark\sql\readwriter.py", line 1586, in saveAsTable
    self._jwrite.saveAsTable(name)
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl-venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl-venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Couldn't find a catalog to handle the identifier bsrc_d.log_202505.process_log.
------------------------------------------------------------------------------------------- Captured log call -------------------------------------------------------------------------------------------- 
ERROR    betl_src_poc_logger:table_logging.py:37 Error writing to process log table log_202505.process_log
Traceback (most recent call last):
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl\src\abnamro_bsrc_etl\utils\table_logging.py", line 33, in write_to_log
    log_entry_df.write.mode("append").saveAsTable(
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl-venv\lib\site-packages\pyspark\sql\readwriter.py", line 1586, in saveAsTable
    self._jwrite.saveAsTable(name)
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl-venv\lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "C:\Users\B25712\bsrc-etl-venv\bsrc-etl-venv\lib\site-packages\pyspark\errors\exceptions\captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: Couldn't find a catalog to handle the identifier bsrc_d.log_202505.process_log.
============================================================================================ warnings summary ============================================================================================
..\bsrc-etl-venv\lib\site-packages\holidays\deprecations\v1_incompatibility.py:40
  C:\Users\B25712\bsrc-etl-venv\bsrc-etl-venv\lib\site-packages\holidays\deprecations\v1_incompatibility.py:40: FutureIncompatibilityWarning:

  This is a future version incompatibility warning from Holidays v0.62
  to inform you about an upcoming change in our API versioning strategy that may affect your
  project's dependencies. Starting from version 1.0 onwards, we will be following a loose form of
  Semantic Versioning (SemVer, https://semver.org) to provide clearer communication regarding any
  potential breaking changes.

  This means that while we strive to maintain backward compatibility, there might be occasional
  updates that introduce breaking changes to our API. To ensure the stability of your projects,
  we highly recommend pinning the version of our API that you rely on. You can pin your current
  holidays v0.x dependency (e.g., holidays==0.62) or limit it (e.g., holidays<1.0) in order to
  avoid potentially unwanted upgrade to the version 1.0 when it's released (ETA 2025Q1-Q2).

  If you have any questions or concerns regarding this change, please don't hesitate to reach out
  to us via https://github.com/vacanza/holidays/discussions/1800.

    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html

---------- coverage: platform win32, python 3.10.11-final-0 ----------
Name                                                             Stmts   Miss  Cover   Missing
----------------------------------------------------------------------------------------------
src\__init__.py                                                      0      0   100%
src\abnamro_bsrc_etl\__init__.py                                     0      0   100%
src\abnamro_bsrc_etl\config\__init__.py                              0      0   100%
src\abnamro_bsrc_etl\config\business_logic.py                       52      0   100%
src\abnamro_bsrc_etl\config\constants.py                             1      0   100%
src\abnamro_bsrc_etl\config\exceptions.py                           21     21     0%   10-52
src\abnamro_bsrc_etl\config\process.py                               4      4     0%   7-10
src\abnamro_bsrc_etl\config\schema.py                                4      4     0%   3-52
src\abnamro_bsrc_etl\dq\__init__.py                                  0      0   100%
src\abnamro_bsrc_etl\dq\dq_validation.py                           141    132     6%   47-112, 166-181, 197-249, 265-286, 318-350, 372-392, 414-486
src\abnamro_bsrc_etl\extract\__init__.py                             0      0   100%
src\abnamro_bsrc_etl\extract\master_data_sql.py                     90     71    21%   31-33, 37-38, 42, 59-88, 109-137, 149-156, 162-164, 188, 222-231, 285-300, 329-356
src\abnamro_bsrc_etl\month_setup\__init__.py                         0      0   100%
src\abnamro_bsrc_etl\month_setup\dial_derive_snapshotdate.py        32     27    16%   12-18, 28-37, 47-50, 69-85
src\abnamro_bsrc_etl\month_setup\metadata_log_tables.py             32     25    22%   23-84, 159, 166-203
src\abnamro_bsrc_etl\month_setup\setup_new_month.py                 13     13     0%   17-84
src\abnamro_bsrc_etl\scripts\__init__.py                             0      0   100%
src\abnamro_bsrc_etl\scripts\dial_check_delayed_files.py            20     20     0%   4-71
src\abnamro_bsrc_etl\scripts\dial_staging_process.py                54     54     0%   4-270
src\abnamro_bsrc_etl\scripts\export_tine_tables.py                   1      1     0%   7
src\abnamro_bsrc_etl\scripts\new_month_setup.py                      4      4     0%   8-18
src\abnamro_bsrc_etl\scripts\nonssf_staging_process.py              49     49     0%   10-224
src\abnamro_bsrc_etl\scripts\run_mapping.py                         15     15     0%   26-108
src\abnamro_bsrc_etl\scripts\ssf_staging_process.py                 48     48     0%   14-222
src\abnamro_bsrc_etl\staging\__init__.py                             0      0   100%
src\abnamro_bsrc_etl\staging\extract_base.py                        65     17    74%   156, 162-164, 304-312, 343-352, 387-404, 420
src\abnamro_bsrc_etl\staging\extract_dial_data.py                   65     65     0%   16-360
src\abnamro_bsrc_etl\staging\extract_nonssf_data.py                140     70    50%   81-98, 151-173, 224-229, 262-322, 333-355, 414-448, 463, 479, 502-524
src\abnamro_bsrc_etl\staging\extract_ssf_data.py                   164    164     0%   26-620
src\abnamro_bsrc_etl\staging\status.py                              57      5    91%   18, 53-54, 107, 151
src\abnamro_bsrc_etl\transform\__init__.py                           0      0   100%
src\abnamro_bsrc_etl\transform\table_write_and_comment.py           72     72     0%   14-237
src\abnamro_bsrc_etl\transform\transform_business_logic_sql.py       6      6     0%   6-25
src\abnamro_bsrc_etl\utils\__init__.py                               0      0   100%
src\abnamro_bsrc_etl\utils\alias_util.py                            13     13     0%   10-109
src\abnamro_bsrc_etl\utils\export_parquet.py                        15     12    20%   38-57, 67-68
src\abnamro_bsrc_etl\utils\get_dbutils.py                            2      0   100%
src\abnamro_bsrc_etl\utils\get_env.py                               10      0   100%
src\abnamro_bsrc_etl\utils\logging_util.py                           6      0   100%
src\abnamro_bsrc_etl\utils\parameter_utils.py                       23     19    17%   33-53, 79-84, 105-116
src\abnamro_bsrc_etl\utils\parse_yaml.py                            22     22     0%   11-127
src\abnamro_bsrc_etl\utils\sources_util.py                          52     52     0%   7-218
src\abnamro_bsrc_etl\utils\table_logging.py                         14      3    79%   54-56
src\abnamro_bsrc_etl\utils\table_schema.py                           3      3     0%   8-16
src\abnamro_bsrc_etl\utils\transformations_util.py                  17     12    29%   20-25, 39, 51, 65-68
src\abnamro_bsrc_etl\validate\__init__.py                            0      0   100%
src\abnamro_bsrc_etl\validate\base.py                                4      4     0%   4-7
src\abnamro_bsrc_etl\validate\expressions.py                        27     27     0%   15-75
src\abnamro_bsrc_etl\validate\run_all.py                             7      7     0%   12-48
src\abnamro_bsrc_etl\validate\sources.py                            29     29     0%   7-67
src\abnamro_bsrc_etl\validate\transformations.py                   192    192     0%   21-593
src\abnamro_bsrc_etl\validate\validate_sql.py                       54     54     0%   13-130
src\abnamro_bsrc_etl\validate\yaml.py                               18     18     0%   3-34
----------------------------------------------------------------------------------------------
TOTAL                                                             1658   1354    18%
Coverage HTML written to dir htmlcov

======================================================================================== short test summary info ========================================================================================= 
FAILED test/staging/test_extract_non_ssf_data.py::test_extract_non_ssf_data[202505-test-container] - AssertionError: assert 'Deadline not reached for TEST_NON_SSF_V1' in 'INFO     betl_src_poc_logger:extract_base.py:205 FileDeliveryStatus: Received / Placed for TEST_NON_SSF_V3\nINFO     ...reached....
FAILED test/staging/test_extract_non_ssf_data.py::test_append_to_process_log[202505-test-container] - AssertionError: Expected 'write_to_log' to have been called once. Called 0 times.
================================================================================ 2 failed, 1 passed, 1 warning in 50.48s ================================================================================= 
SUCCESS: The process with PID 15448 (child process of PID 8272) has been terminated.
SUCCESS: The process with PID 8272 (child process of PID 10224) has been terminated.
SUCCESS: The process with PID 10224 (child process of PID 16696) has been terminated.
